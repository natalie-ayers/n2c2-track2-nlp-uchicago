{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Readmission using Pytorch's Logistic Regression and DAN\n",
    "\n",
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple, Type\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchmetrics import Accuracy, Precision\n",
    "\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Logistic regression binary classification model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        # Parameters\n",
    "        num_features : `int`, required.\n",
    "            Number of the features.\n",
    "        # Returns\n",
    "            `None`\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Hw-TODO: Add a linear layer to weight the features.\n",
    "        self.linear = nn.Linear(in_features=num_features, out_features=1, bias=True)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Returns the logits of the model given features. \n",
    "        Note that model predictions should be either 0 or 1 based on a threshold.\n",
    "        # Parameters\n",
    "        features : `torch.FloatTensor`, required.\n",
    "            The tensor of features with the shape (batch_size, num_of_features)\n",
    "        # Returns\n",
    "        probs : `torch.FloatTensor`, required.\n",
    "            The tensor of probabilities with the shape (batch_size, 1) or (batch_size,)\n",
    "        \"\"\"\n",
    "        # Hw-TODO: Use `self.linear` you created in `__init__`\n",
    "        #          and appropriate nonlinearity/activation-function to compute\n",
    "        #          and return the probabilities of belonging to a class in the logistic regression.\n",
    "        out = self.linear(features)\n",
    "        out = self.dropout(out)\n",
    "        probs = torch.sigmoid(out)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassificationLModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save arguments to `hparams` attribute, see the doc [here](https://pytorch-lightning.readthedocs.io/en/latest/common/hyperparameters.html).\n",
    "        self.save_hyperparameters()\n",
    "        data_dir = Path(self.hparams.data_dir)\n",
    "        #self.hparams.vocab = json.load(\n",
    "        #    open(data_dir.joinpath(self.hparams.vocab_filename)))\n",
    "        #self.hparams.vocab_size = len(self.hparams.vocab)\n",
    "\n",
    "        self.model = self.get_model()\n",
    "        self.step_count = 0\n",
    "        self.accuracy = Accuracy()\n",
    "        self.pr = Precision(threshold=0.5,average='macro',num_classes=1,multiclass=False)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = self.batch2labels(batch)\n",
    "        probs = self(**input)\n",
    "        probs = probs.squeeze()\n",
    "        # Hw-TODO: Given probs in shape (batch_size,)\n",
    "        #          and labels of the same shape,\n",
    "        #          compute the binary cross entropy loss.\n",
    "        loss = nn.functional.binary_cross_entropy(probs, labels)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', self.accuracy(probs, labels.int()), prog_bar=True)\n",
    "        self.log('train_pr', self.pr(probs, labels.int()), prog_bar=True)\n",
    "        output_dict = {'loss': loss}\n",
    "        return output_dict\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = self.batch2labels(batch)\n",
    "        probs = self(**input)\n",
    "        probs = probs.squeeze()\n",
    "\n",
    "        # Hw-TODO: Given probs in shape (batch_size,)\n",
    "        #          and labels of the same shape,\n",
    "        #          compute the binary cross entropy loss.\n",
    "        loss = nn.functional.binary_cross_entropy(probs, labels)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', self.accuracy(probs, labels.int()))\n",
    "        self.log('val_pr', self.pr(probs, labels.int()))\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = self.batch2labels(batch)\n",
    "        probs = self(**input)\n",
    "        probs = probs.squeeze()\n",
    "\n",
    "        # Hw-TODO: Given probs in shape (batch_size,)\n",
    "        #          and labels of the same shape,\n",
    "        #          compute the binary cross entropy loss.\n",
    "        loss = nn.functional.binary_cross_entropy(probs, labels)\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', self.accuracy(probs, labels.int()))\n",
    "        self.log('test_pr', self.pr(probs, labels.int()))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.optimizer == 'sgd':\n",
    "            optimizer = torch.optim.SGD(self.model.parameters(),\n",
    "                                        lr=self.hparams.learning_rate)\n",
    "        elif self.hparams.optimizer == 'adam':\n",
    "            if self.hparams.l2_regularization:\n",
    "                optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                         lr=self.hparams.learning_rate,\n",
    "                                         weight_decay=1e-5)\n",
    "            else:\n",
    "                optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                         lr=self.hparams.learning_rate)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader('train', self.hparams.train_batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader('dev', self.hparams.eval_batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.get_dataloader('test', self.hparams.eval_batch_size, shuffle=False)\n",
    "\n",
    "    def get_model(self) -> nn.Module:\n",
    "        # To be overridden by inherited classes.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def batch2input(self, batch: Tuple[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # To be overridden by inherited classes.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def batch2labels(self, batch: Tuple[torch.Tensor]) -> torch.Tensor:\n",
    "        # To be overridden by inherited classes.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_dataloader(self,\n",
    "                       split: str,\n",
    "                       batch_size: int,\n",
    "                       shuffle: bool = False) -> DataLoader:\n",
    "        # To be overridden by inherited classes.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def add_model_specific_args(cls, parser: ArgumentParser) -> ArgumentParser:\n",
    "        \"\"\"\n",
    "        Add arguments to the parser and return the parser.\n",
    "        \"\"\"\n",
    "        # Required arguments:\n",
    "        parser.add_argument('--vocab_filename',\n",
    "                            default=None,\n",
    "                            type=str,\n",
    "                            required=False,  # changing to false since we're not using vocab\n",
    "                            help=\"File name of the feature.\")\n",
    "        # Optional arguments:\n",
    "        parser.add_argument('--optimizer',\n",
    "                            default='adam',\n",
    "                            type=str,\n",
    "                            help=\"The optimizer to use, such as sgd or adam.\")\n",
    "        parser.add_argument('--learning_rate',\n",
    "                            default=1e-3,\n",
    "                            type=float,\n",
    "                            help=\"The initial learning rate for training.\")\n",
    "        parser.add_argument('--l2_regularization',\n",
    "                            default=False,\n",
    "                            help=\"Whether to add L2 regularization.\")\n",
    "        parser.add_argument('--max_epochs',\n",
    "                            default=10,\n",
    "                            type=int,\n",
    "                            help=\"The number of epochs to train your model.\")\n",
    "        parser.add_argument('--train_batch_size', default=32, type=int)\n",
    "        parser.add_argument('--eval_batch_size', default=32, type=int)\n",
    "        parser.add_argument('--seed',\n",
    "                            type=int,\n",
    "                            default=42,\n",
    "                            help=\"The random seed for initialization\")\n",
    "        parser.add_argument('--do_train',\n",
    "                            action=\"store_true\",\n",
    "                            default=True,\n",
    "                            help=\"Whether to run training.\")\n",
    "        parser.add_argument('--do_predict',\n",
    "                            action=\"store_true\",\n",
    "                            help=\"Whether to run predictions on the test set.\")\n",
    "        parser.add_argument('--data_dir',\n",
    "                            default=\"data\",\n",
    "                            type=str,\n",
    "                            help=\"The input data dir. Should contain the training files.\")\n",
    "        parser.add_argument('--output_dir',\n",
    "                            type=str,\n",
    "                            help=(\"The output directory where the model predictions \"\n",
    "                                  \"and checkpoints will be written.\"))\n",
    "        # NOTE: Set --gpus 0 or change the default value to 0 if not using GPUS.\n",
    "        # See this [link](https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu.html) for usage of this argument.\n",
    "        parser.add_argument('--gpus',\n",
    "                            default=1,\n",
    "                            type=int,\n",
    "                            help=\"The number of GPUs allocated for this, 0 meaning none\")\n",
    "        parser.add_argument('--num_workers',\n",
    "                            default=8,\n",
    "                            type=int,\n",
    "                            help=\"Config `DataLoader` of pytorch\")\n",
    "        return parser\n",
    "\n",
    "\n",
    "def generic_train(args: argparse.Namespace,\n",
    "                  model_class: Type[pl.LightningModule]) -> Dict:\n",
    "    \"\"\"\n",
    "        Train (and optionally predict) and return dict results.\n",
    "        # Parameters\n",
    "        args : `argparse.Namespace`, required.\n",
    "            Configuration of the training and the model\n",
    "        model_class : `Type[pl.LightningModule]`, required.\n",
    "            Class of the model to be trained.\n",
    "        # Returns\n",
    "        A `dict` object containing the following keys and types.\n",
    "            trainer: `pl.Trainer`\n",
    "            model: `pl.LightningModule`\n",
    "            val_results_best: `list[dict]`\n",
    "                If `args.do_predict==True`\n",
    "            test_results_best: `list[dict]`\n",
    "                If `args.do_predict==True`\n",
    "            best_model_path: `Path`\n",
    "                Path to the checkpoint of the best model.\n",
    "        \"\"\"\n",
    "    pl.seed_everything(args.seed)\n",
    "\n",
    "    tensorboard_log_dir = Path(args.output_dir).joinpath('tensorboard_logs')\n",
    "    tensorboard_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Tensorboard logger\n",
    "    tensorboard_logger = pl_loggers.TensorBoardLogger(\n",
    "        save_dir=tensorboard_log_dir,\n",
    "        version='version_' + datetime.now().strftime('%Y%m%d-%H%M%S'),\n",
    "        name='',\n",
    "        default_hp_metric=True)\n",
    "    # Checkpoint callback\n",
    "    checkpoint_dir = Path(args.output_dir).joinpath(tensorboard_logger.version,\n",
    "                                                    'checkpoints')\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=checkpoint_dir,\n",
    "                                                       filename='{epoch}-{val_acc:.2f}',\n",
    "                                                       monitor='val_pr',\n",
    "                                                       mode='max',\n",
    "                                                       save_top_k=1,\n",
    "                                                       verbose=True)\n",
    "\n",
    "    dict_args = vars(args)\n",
    "    model = model_class(**dict_args)\n",
    "    trainer = pl.Trainer.from_argparse_args(args,\n",
    "                                            logger=tensorboard_logger,\n",
    "                                            callbacks=[checkpoint_callback])\n",
    "\n",
    "    output_dict = {'trainer': trainer, 'model': model}\n",
    "\n",
    "    if args.do_train:\n",
    "        trainer.fit(model=model)\n",
    "        # Track model performance under differnt hparams settings in \"Hparams\" of TensorBoard\n",
    "        tensorboard_logger.log_hyperparams(\n",
    "            params=model.hparams,\n",
    "            metrics={'hp_metric': checkpoint_callback.best_model_score.item()})\n",
    "        tensorboard_logger.save()\n",
    "\n",
    "        # Save the best model to `best_model.ckpt`\n",
    "        best_model_path = checkpoint_dir.joinpath('best_model.ckpt')\n",
    "        logger.info(f\"Copy best model from {checkpoint_callback.best_model_path} \"\n",
    "                    f\"to {best_model_path}.\")\n",
    "        shutil.copy(checkpoint_callback.best_model_path, best_model_path)\n",
    "\n",
    "        output_dict.update({\n",
    "            'trainer': trainer,\n",
    "            'model': model,\n",
    "            'best_model_path': best_model_path\n",
    "        })\n",
    "\n",
    "    # Optionally, predict on test set.\n",
    "    if args.do_predict:\n",
    "        best_model_path = checkpoint_dir.joinpath('best_model.ckpt')\n",
    "        model = model.load_from_checkpoint(best_model_path)\n",
    "        val_results_best = trainer.validate(model, verbose=True)\n",
    "        test_results_best = trainer.test(model, verbose=True)\n",
    "        print(\"Validation accuracy on the best model: {: .4f}\".format(\n",
    "            val_results_best[0]['val_acc']))\n",
    "        print(\"Validation precision on the best model: {: .4f}\".format(\n",
    "            val_results_best[0]['val_pr']))\n",
    "        print('val_results_best all',val_results_best[0])\n",
    "        #print('val_results_best_trulyall',val_results_best)\n",
    "        print(\"Test accuracy on the best model: {: .4f}\".format(\n",
    "            test_results_best[0]['test_acc']))\n",
    "        print(\"Test precision on the best model: {: .4f}\".format(\n",
    "            test_results_best[0]['test_pr']))\n",
    "        output_dict.update({\n",
    "            'val_results_best': val_results_best,\n",
    "            'test_results_best': test_results_best,\n",
    "        })\n",
    "\n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureBasedBinaryClassificationLModule(BinaryClassificationLModule):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def get_model(self) -> nn.Module:\n",
    "        return LogisticRegressionModel(num_features=764)\n",
    "\n",
    "    def batch2input(self, batch):\n",
    "        return {'features': batch[0]}\n",
    "\n",
    "    def batch2labels(self, batch):\n",
    "        return batch[1]\n",
    "\n",
    "    def get_dataloader(self,\n",
    "                       split: str,\n",
    "                       batch_size: int,\n",
    "                       shuffle: bool = False) -> DataLoader:\n",
    "        # NOTE: In order to use different features, change feature_name by\n",
    "        # passing `--feature_name <feature_name>` in the training loop in\n",
    "        # the cell below, or revise the code here for correct paths if needed.\n",
    "        data_dir = Path(self.hparams.data_dir)\n",
    "        features_filepath = data_dir.joinpath(\n",
    "            f\"{split}_{self.hparams.feature_name}_features.npz\")\n",
    "        labels_filepath = data_dir.joinpath(f\"{split}_{self.hparams.label_name}_labels.npz\")\n",
    "        features = sparse.load_npz(features_filepath).todense()\n",
    "        print('features shape:',features.shape)\n",
    "        #labels = np.load(labels_filepath, allow_pickle=True)[\"arr_0\"]\n",
    "        labels = np.asarray(sparse.load_npz(labels_filepath).todense()).ravel()\n",
    "        print('labels shape:',labels.shape)\n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(features).float(),\n",
    "            torch.from_numpy(labels).float())\n",
    "\n",
    "        logger.info(f\"Loading {split} features and labels \"\n",
    "                    f\"from {features_filepath} and {labels_filepath}\")\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=shuffle,\n",
    "                                                  num_workers=self.hparams.num_workers)\n",
    "        return data_loader\n",
    "\n",
    "    @classmethod\n",
    "    def add_model_specific_args(cls, parser: ArgumentParser) -> ArgumentParser:\n",
    "        parser = super().add_model_specific_args(parser)\n",
    "        # Required arguments:\n",
    "        parser.add_argument('--feature_name',\n",
    "                            default=None,\n",
    "                            type=str,\n",
    "                            required=True,\n",
    "                            help=\"Name of the feature\")\n",
    "                            \n",
    "        parser.add_argument('--label_name',\n",
    "                            default=None,\n",
    "                            type=str,\n",
    "                            required=True,\n",
    "                            help=\"Name of the label\")\n",
    "        # Optional arguments:\n",
    "        parser.add_argument('--task',\n",
    "                            default='featurebinarycls',\n",
    "                            type=str,\n",
    "                            help=\"Name of the task.\")\n",
    "        return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.asarray(sparse.load_npz('./data/train_labels.npz').todense()).ravel()\n",
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                    | Params\n",
      "-----------------------------------------------------\n",
      "0 | model    | LogisticRegressionModel | 765   \n",
      "1 | accuracy | Accuracy                | 0     \n",
      "2 | pr       | Precision               | 0     \n",
      "-----------------------------------------------------\n",
      "765       Trainable params\n",
      "0         Non-trainable params\n",
      "765       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed arguments: Namespace(vocab_filename=None, optimizer='adam', learning_rate=0.001, l2_regularization=False, max_epochs=10, train_batch_size=32, eval_batch_size=32, seed=42, do_train=True, do_predict=True, data_dir='data', output_dir='output/ftrlogistic', gpus=1, num_workers=8, feature_name='roberta', label_name='readmit', task='featurebinarycls')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26881e19f3248b89836d70e89071169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 23:46:53 - INFO - __main__ - Loading dev features and labels from data/dev_roberta_features.npz and data/dev_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "2022-03-14 23:46:54 - INFO - __main__ - Loading train features and labels from data/train_roberta_features.npz and data/train_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (34416, 764)\n",
      "labels shape: (34416,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd765a10a0564b0a8bf1f9e9f3ee2457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30347ee314c84bb2b1020e09610b7635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1075: val_pr reached 0.01125 (best 0.01125), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/ftrlogistic/version_20220314-234653/checkpoints/epoch=0-val_acc=0.93.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa23d51424b4c949b395006e6e3cebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 2151: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76269ac542f8409db87f3a3f6129a503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 3227: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f74f47291044cdebc4ca9b979736f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 4303: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3ba44f649d4699872e442d5157f373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 5379: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809e885c321e44c281fe4f8098cc5e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 6455: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3651f3643d4eff91bc2543e60da2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 7531: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b674ad688f441f2b839ff21af619d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 8607: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4169fae9cc4a1d9200fea684c6970e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 9683: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167da8700ecc4c098543e289d98b2f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 10759: val_pr was not in top 1\n",
      "2022-03-14 23:50:46 - INFO - __main__ - Copy best model from /mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/ftrlogistic/version_20220314-234653/checkpoints/epoch=0-val_acc=0.93.ckpt to output/ftrlogistic/version_20220314-234653/checkpoints/best_model.ckpt.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2022-03-14 23:50:47 - INFO - __main__ - Loading dev features and labels from data/dev_roberta_features.npz and data/dev_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39aba87656384e39947dbd87290a8773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_acc': 0.9340190291404724,\n",
      " 'val_loss': 6.598099708557129,\n",
      " 'val_pr': 0.0112496018409729}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 23:50:52 - INFO - __main__ - Loading test features and labels from data/test_roberta_features.npz and data/test_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637e915402f84224a3df00b82da62164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9347162842750549,\n",
      " 'test_loss': 6.479560852050781,\n",
      " 'test_pr': 0.00790261197835207}\n",
      "--------------------------------------------------------------------------------\n",
      "Validation accuracy on the best model:  0.9340\n",
      "Validation precision on the best model:  0.0112\n",
      "val_results_best all {'val_loss': 6.598099708557129, 'val_acc': 0.9340190291404724, 'val_pr': 0.0112496018409729}\n",
      "Test accuracy on the best model:  0.9347\n",
      "Test precision on the best model:  0.0079\n"
     ]
    }
   ],
   "source": [
    "# num_classes=1, multiclass=False, dropout=0.6\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load hyperparameters\n",
    "parser = ArgumentParser()\n",
    "parser = FeatureBasedBinaryClassificationLModule.add_model_specific_args(parser)\n",
    "\n",
    "# NOTE: You should replace `unigram_binary` in the assignment statement of `args_str =...`\n",
    "# with whatever feature that you are experimented with.\n",
    "# You can also configure other options listed in the method of add_model_specific_args of\n",
    "# the pytorch-lightning model `FeatureBasedBinaryClassificationLModule`.\n",
    "args_str = (\"--feature_name roberta --max_epochs 10 --label_name readmit \"\n",
    "            \"--output_dir output/ftrlogistic --optimizer adam --do_train --do_predict\")\n",
    "\n",
    "args = parser.parse_args(args_str.split())\n",
    "\n",
    "# If output_dir not provided, a folder is generated\n",
    "if args.output_dir is None:\n",
    "    args.output_dir = str(\n",
    "        Path('output').joinpath(\n",
    "            f\"{args.task}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"))\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Parsed arguments: {args}\")\n",
    "\n",
    "training_outout = generic_train(args=args,\n",
    "                                model_class=FeatureBasedBinaryClassificationLModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                    | Params\n",
      "-----------------------------------------------------\n",
      "0 | model    | LogisticRegressionModel | 765   \n",
      "1 | accuracy | Accuracy                | 0     \n",
      "2 | pr       | Precision               | 0     \n",
      "-----------------------------------------------------\n",
      "765       Trainable params\n",
      "0         Non-trainable params\n",
      "765       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed arguments: Namespace(vocab_filename=None, optimizer='adam', learning_rate=0.001, l2_regularization='True', max_epochs=10, train_batch_size=32, eval_batch_size=32, seed=42, do_train=True, do_predict=True, data_dir='data', output_dir='output/ftrlogistic', gpus=1, num_workers=8, feature_name='roberta', label_name='readmit', task='featurebinarycls')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2437c92ca240edb4d654d0d39d4e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 23:51:27 - INFO - __main__ - Loading dev features and labels from data/dev_roberta_features.npz and data/dev_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "2022-03-14 23:51:28 - INFO - __main__ - Loading train features and labels from data/train_roberta_features.npz and data/train_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (34416, 764)\n",
      "labels shape: (34416,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1db8fb40224aa6b325d5657b749a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d4d7b90af14a3e86be39ee1c133ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1075: val_pr reached 0.01497 (best 0.01497), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/ftrlogistic/version_20220314-235127/checkpoints/epoch=0-val_acc=0.93.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73378a87ddab45ff92c577c0b35bc8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 2151: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee8525201984275b6a600f99545873c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 3227: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aae0a4ac07049f9a70e626d71852563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 4303: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bea35a2b35418b890a962a320594da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 5379: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c33826898e42129dcd2efa86ce69f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 6455: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3240a67a8f8e499e88a6298d3f243582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 7531: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e355728fb0e442e6aab0dc18c6593d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 8607: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0934f91624154dcf9a6cd005b7bf8ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 9683: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f881f6d1064d3f909de6c94eddb44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 10759: val_pr was not in top 1\n",
      "2022-03-14 23:55:26 - INFO - __main__ - Copy best model from /mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/ftrlogistic/version_20220314-235127/checkpoints/epoch=0-val_acc=0.93.ckpt to output/ftrlogistic/version_20220314-235127/checkpoints/best_model.ckpt.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2022-03-14 23:55:27 - INFO - __main__ - Loading dev features and labels from data/dev_roberta_features.npz and data/dev_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eecd3fcb86f4abe81aaeb53fd7d4840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_acc': 0.9259130358695984,\n",
      " 'val_loss': 7.400588512420654,\n",
      " 'val_pr': 0.014968477189540863}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 23:55:31 - INFO - __main__ - Loading test features and labels from data/test_roberta_features.npz and data/test_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fd8ec44e784fc2a637d5307a3b6f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.926174521446228,\n",
      " 'test_loss': 7.374264717102051,\n",
      " 'test_pr': 0.00883233081549406}\n",
      "--------------------------------------------------------------------------------\n",
      "Validation accuracy on the best model:  0.9259\n",
      "Validation precision on the best model:  0.0150\n",
      "val_results_best all {'val_loss': 7.400588512420654, 'val_acc': 0.9259130358695984, 'val_pr': 0.014968477189540863}\n",
      "Test accuracy on the best model:  0.9262\n",
      "Test precision on the best model:  0.0088\n"
     ]
    }
   ],
   "source": [
    "# dropout=0.6, using l2 regularization\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load hyperparameters\n",
    "parser = ArgumentParser()\n",
    "parser = FeatureBasedBinaryClassificationLModule.add_model_specific_args(parser)\n",
    "\n",
    "# NOTE: You should replace `unigram_binary` in the assignment statement of `args_str =...`\n",
    "# with whatever feature that you are experimented with.\n",
    "# You can also configure other options listed in the method of add_model_specific_args of\n",
    "# the pytorch-lightning model `FeatureBasedBinaryClassificationLModule`.\n",
    "args_str = (\"--feature_name roberta --max_epochs 10 --label_name readmit \"\n",
    "            \"--output_dir output/ftrlogistic --optimizer adam --do_train --do_predict \"\n",
    "            \"--l2_regularization True \")\n",
    "\n",
    "args = parser.parse_args(args_str.split())\n",
    "\n",
    "# If output_dir not provided, a folder is generated\n",
    "if args.output_dir is None:\n",
    "    args.output_dir = str(\n",
    "        Path('output').joinpath(\n",
    "            f\"{args.task}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"))\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Parsed arguments: {args}\")\n",
    "\n",
    "training_outout = generic_train(args=args,\n",
    "                                model_class=FeatureBasedBinaryClassificationLModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                    | Params\n",
      "-----------------------------------------------------\n",
      "0 | model    | LogisticRegressionModel | 765   \n",
      "1 | accuracy | Accuracy                | 0     \n",
      "2 | pr       | Precision               | 0     \n",
      "-----------------------------------------------------\n",
      "765       Trainable params\n",
      "0         Non-trainable params\n",
      "765       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed arguments: Namespace(vocab_filename=None, optimizer='sgd', learning_rate=0.001, l2_regularization=False, max_epochs=10, train_batch_size=32, eval_batch_size=32, seed=42, do_train=True, do_predict=True, data_dir='data', output_dir='output/ftrlogistic', gpus=1, num_workers=8, feature_name='roberta', label_name='readmit', task='featurebinarycls')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8ae21b208a4f25bdcf73b7ed4f88ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 23:59:30 - INFO - __main__ - Loading dev features and labels from data/dev_roberta_features.npz and data/dev_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "2022-03-14 23:59:31 - INFO - __main__ - Loading train features and labels from data/train_roberta_features.npz and data/train_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (34416, 764)\n",
      "labels shape: (34416,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6033c0d53a940ecacfe0606beab18dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab7d9d615f9409892ec5614170faa0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1075: val_pr reached 0.00000 (best 0.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/ftrlogistic/version_20220314-235930/checkpoints/epoch=0-val_acc=0.96.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e8cebdb5f445dd859e0acf861ad75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 2151: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f036fe1125142d0b0766fb5de3b0f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 3227: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9cc1ad8f224746b2b57f7582ea586a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 4303: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1a7ab7ad304e8ea122e4fa64b0a976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 5379: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7dea8eafa343169c9e6292c2d7c34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 6455: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ea47db72684f599c22ffb578c9ebb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 7531: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae08de741fe84222b0bdac52726ac7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 8607: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f2fb6ab19c46e8b61dcc3e0300eb2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 9683: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c77eb852944823b85ca9fb5ddde4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 10759: val_pr was not in top 1\n",
      "2022-03-15 00:03:24 - INFO - __main__ - Copy best model from /mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/ftrlogistic/version_20220314-235930/checkpoints/epoch=0-val_acc=0.96.ckpt to output/ftrlogistic/version_20220314-235930/checkpoints/best_model.ckpt.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2022-03-15 00:03:24 - INFO - __main__ - Loading dev features and labels from data/dev_roberta_features.npz and data/dev_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f3d38aca5c45b58c3ba6b1bb50c2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_acc': 0.9569423794746399, 'val_loss': 4.305761337280273, 'val_pr': 0.0}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 00:03:29 - INFO - __main__ - Loading test features and labels from data/test_roberta_features.npz and data/test_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574e2f49e14b4394be3de64b1aeccfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9591214060783386, 'test_loss': 4.0878586769104, 'test_pr': 0.0}\n",
      "--------------------------------------------------------------------------------\n",
      "Validation accuracy on the best model:  0.9569\n",
      "Validation precision on the best model:  0.0000\n",
      "val_results_best all {'val_loss': 4.305761337280273, 'val_acc': 0.9569423794746399, 'val_pr': 0.0}\n",
      "Test accuracy on the best model:  0.9591\n",
      "Test precision on the best model:  0.0000\n"
     ]
    }
   ],
   "source": [
    "# dropout=0.6, using sgd\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load hyperparameters\n",
    "parser = ArgumentParser()\n",
    "parser = FeatureBasedBinaryClassificationLModule.add_model_specific_args(parser)\n",
    "\n",
    "# NOTE: You should replace `unigram_binary` in the assignment statement of `args_str =...`\n",
    "# with whatever feature that you are experimented with.\n",
    "# You can also configure other options listed in the method of add_model_specific_args of\n",
    "# the pytorch-lightning model `FeatureBasedBinaryClassificationLModule`.\n",
    "args_str = (\"--feature_name roberta --max_epochs 10 --label_name readmit \"\n",
    "            \"--output_dir output/ftrlogistic --optimizer sgd --do_train --do_predict \"\n",
    "            )\n",
    "\n",
    "args = parser.parse_args(args_str.split())\n",
    "\n",
    "# If output_dir not provided, a folder is generated\n",
    "if args.output_dir is None:\n",
    "    args.output_dir = str(\n",
    "        Path('output').joinpath(\n",
    "            f\"{args.task}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"))\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Parsed arguments: {args}\")\n",
    "\n",
    "training_outout = generic_train(args=args,\n",
    "                                model_class=FeatureBasedBinaryClassificationLModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                    | Params\n",
      "-----------------------------------------------------\n",
      "0 | model    | LogisticRegressionModel | 765   \n",
      "1 | accuracy | Accuracy                | 0     \n",
      "2 | pr       | Precision               | 0     \n",
      "-----------------------------------------------------\n",
      "765       Trainable params\n",
      "0         Non-trainable params\n",
      "765       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed arguments: Namespace(vocab_filename=None, optimizer='adam', learning_rate=0.001, l2_regularization='True', max_epochs=20, train_batch_size=32, eval_batch_size=32, seed=42, do_train=True, do_predict=True, data_dir='data', output_dir='output/ftrlogistic', gpus=1, num_workers=8, feature_name='roberta', label_name='readmit', task='featurebinarycls')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb6d9248fd34ce79be7e586df0313d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 00:13:36 - INFO - __main__ - Loading dev features and labels from data/dev_roberta_features.npz and data/dev_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "2022-03-15 00:13:38 - INFO - __main__ - Loading train features and labels from data/train_roberta_features.npz and data/train_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (34416, 764)\n",
      "labels shape: (34416,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e2c3dd15ab44a287c8b18333e5451b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8536c59560478ab1043d3fe05f1014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1075: val_pr reached 0.01497 (best 0.01497), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/ftrlogistic/version_20220315-001335/checkpoints/epoch=0-val_acc=0.93.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6567252c08641c88e82f00f4198e483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 2151: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5996b606c4134775a87784c274bced57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 3227: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8cd4ac78f04f48b44c59f01b4742d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 4303: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a405eaa55ce14d95a7075f0f49ed6988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 5379: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b54f96cfc744a9867550eab7496292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 6455: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558c2928267e4116916cedd7977489cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 7531: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df68c495707c49b4be0fa3bbf5b5ad84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 8607: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47a0c4b851142b2811441675a021094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 9683: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5257133ef2d94060ad0142dbe2b8a392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 10759: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a9ac1b6166454482e620da922e8dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 11835: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fcebf165a4405a9373fb3561103fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 12911: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4df14e9b60d44caa8fbf2e2cc8f2589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 13987: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e425aa47afe64384b253c697724a3328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 15063: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195f81eb846d40ad887e4f7b39784e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 16139: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f0eff1c9964ace928afa292357c4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 17215: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848ccb9ea4664754b348d22bc8af6f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 18291: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065b023218d84598b04d4294563ff325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 19367: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0e9a6e7bc34edcbe7ae93c9fe056ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 20443: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32fd99fd77b420aad31ba62dfa7b5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 21519: val_pr was not in top 1\n",
      "2022-03-15 00:21:37 - INFO - __main__ - Copy best model from /mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/ftrlogistic/version_20220315-001335/checkpoints/epoch=0-val_acc=0.93.ckpt to output/ftrlogistic/version_20220315-001335/checkpoints/best_model.ckpt.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2022-03-15 00:21:37 - INFO - __main__ - Loading dev features and labels from data/dev_roberta_features.npz and data/dev_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cccd5616f340c2ab8bf415cf850dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_acc': 0.9259130358695984,\n",
      " 'val_loss': 7.400588512420654,\n",
      " 'val_pr': 0.014968477189540863}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 00:21:42 - INFO - __main__ - Loading test features and labels from data/test_roberta_features.npz and data/test_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77085792dad4495880d1413b314b33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.926174521446228,\n",
      " 'test_loss': 7.374264717102051,\n",
      " 'test_pr': 0.00883233081549406}\n",
      "--------------------------------------------------------------------------------\n",
      "Validation accuracy on the best model:  0.9259\n",
      "Validation precision on the best model:  0.0150\n",
      "val_results_best all {'val_loss': 7.400588512420654, 'val_acc': 0.9259130358695984, 'val_pr': 0.014968477189540863}\n",
      "Test accuracy on the best model:  0.9262\n",
      "Test precision on the best model:  0.0088\n"
     ]
    }
   ],
   "source": [
    "# dropout=0.6, using l2 regularization, 20 epochs\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load hyperparameters\n",
    "parser = ArgumentParser()\n",
    "parser = FeatureBasedBinaryClassificationLModule.add_model_specific_args(parser)\n",
    "\n",
    "# NOTE: You should replace `unigram_binary` in the assignment statement of `args_str =...`\n",
    "# with whatever feature that you are experimented with.\n",
    "# You can also configure other options listed in the method of add_model_specific_args of\n",
    "# the pytorch-lightning model `FeatureBasedBinaryClassificationLModule`.\n",
    "args_str = (\"--feature_name roberta --max_epochs 20 --label_name readmit \"\n",
    "            \"--output_dir output/ftrlogistic --optimizer adam --do_train --do_predict \"\n",
    "            \"--l2_regularization True \")\n",
    "\n",
    "args = parser.parse_args(args_str.split())\n",
    "\n",
    "# If output_dir not provided, a folder is generated\n",
    "if args.output_dir is None:\n",
    "    args.output_dir = str(\n",
    "        Path('output').joinpath(\n",
    "            f\"{args.task}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"))\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Parsed arguments: {args}\")\n",
    "\n",
    "training_outout = generic_train(args=args,\n",
    "                                model_class=FeatureBasedBinaryClassificationLModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may uncomment and run the commands below to use Tensorboard in a notebook.\n",
    "#%reload_ext  tensorboard\n",
    "#%tensorboard --logdir output/dan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using BioClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                    | Params\n",
      "-----------------------------------------------------\n",
      "0 | model    | LogisticRegressionModel | 765   \n",
      "1 | accuracy | Accuracy                | 0     \n",
      "2 | pr       | Precision               | 0     \n",
      "-----------------------------------------------------\n",
      "765       Trainable params\n",
      "0         Non-trainable params\n",
      "765       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed arguments: Namespace(vocab_filename=None, optimizer='adam', learning_rate=0.001, l2_regularization='True', max_epochs=10, train_batch_size=32, eval_batch_size=32, seed=42, do_train=True, do_predict=True, data_dir='data', output_dir='output/bc_512_logistic', gpus=1, num_workers=8, feature_name='bc_512', label_name='readmit', task='featurebinarycls')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae332cb1ad84379a7a471cce6ee7dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 00:30:56 - INFO - __main__ - Loading dev features and labels from data/dev_bc_512_features.npz and data/dev_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "2022-03-15 00:30:58 - INFO - __main__ - Loading train features and labels from data/train_bc_512_features.npz and data/train_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (34416, 764)\n",
      "labels shape: (34416,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d47a4890ae494d87015e69c8e520d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152b6dbbd9dc47008d2eef84e3c860a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1075: val_pr reached 0.00000 (best 0.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/bc_512_logistic/version_20220315-003056/checkpoints/epoch=0-val_acc=0.96.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a16d60333b44906aaa16c134430dcd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 2151: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a880b64bf264326a14b7a4b5d315055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 3227: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08289e56ff5406f9ab78aeec12b9bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 4303: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deef07af68394fba8980ba9a78d311af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 5379: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3877c2462bf49e09d1131639885f2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 6455: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df3a33b1c994c1aa63355d90447abf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 7531: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbfb10854d5464e99935ae5ef1d6e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 8607: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cecf5b8dfa4609b56b06975a06db04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 9683: val_pr was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3980441d1abe4281a341e4c3086dac72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 10759: val_pr was not in top 1\n",
      "2022-03-15 00:34:57 - INFO - __main__ - Copy best model from /mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/bc_512_logistic/version_20220315-003056/checkpoints/epoch=0-val_acc=0.96.ckpt to output/bc_512_logistic/version_20220315-003056/checkpoints/best_model.ckpt.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2022-03-15 00:34:58 - INFO - __main__ - Loading dev features and labels from data/dev_bc_512_features.npz and data/dev_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da548cd0bec948ea9f8fbc3a87c29837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_acc': 0.9569423794746399, 'val_loss': 4.305761337280273, 'val_pr': 0.0}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 00:35:02 - INFO - __main__ - Loading test features and labels from data/test_bc_512_features.npz and data/test_readmit_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ec3b9a0fb4478e9886f9a6c8ae8b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9591214060783386, 'test_loss': 4.0878586769104, 'test_pr': 0.0}\n",
      "--------------------------------------------------------------------------------\n",
      "Validation accuracy on the best model:  0.9569\n",
      "Validation precision on the best model:  0.0000\n",
      "val_results_best all {'val_loss': 4.305761337280273, 'val_acc': 0.9569423794746399, 'val_pr': 0.0}\n",
      "Test accuracy on the best model:  0.9591\n",
      "Test precision on the best model:  0.0000\n"
     ]
    }
   ],
   "source": [
    "# dropout=0.6, using l2 regularization\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load hyperparameters\n",
    "parser = ArgumentParser()\n",
    "parser = FeatureBasedBinaryClassificationLModule.add_model_specific_args(parser)\n",
    "\n",
    "# NOTE: You should replace `unigram_binary` in the assignment statement of `args_str =...`\n",
    "# with whatever feature that you are experimented with.\n",
    "# You can also configure other options listed in the method of add_model_specific_args of\n",
    "# the pytorch-lightning model `FeatureBasedBinaryClassificationLModule`.\n",
    "args_str = (\"--feature_name bc_512 --max_epochs 10 --label_name readmit \"\n",
    "            \"--output_dir output/bc_512_logistic --optimizer adam --do_train --do_predict \"\n",
    "            \"--l2_regularization True \")\n",
    "\n",
    "args = parser.parse_args(args_str.split())\n",
    "\n",
    "# If output_dir not provided, a folder is generated\n",
    "if args.output_dir is None:\n",
    "    args.output_dir = str(\n",
    "        Path('output').joinpath(\n",
    "            f\"{args.task}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"))\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Parsed arguments: {args}\")\n",
    "\n",
    "training_outout = generic_train(args=args,\n",
    "                                model_class=FeatureBasedBinaryClassificationLModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAveragingNetworksModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 #vocab,\n",
    "                 vocab_size: int,\n",
    "                 word_embedding_size: int,\n",
    "                 hidden_size: int,\n",
    "                 num_intermediate_layers: int,\n",
    "                 dropout_rate: float,\n",
    "                 use_glove: bool = False):\n",
    "        \"\"\"\n",
    "        # Parameters\n",
    "        vocab : `dict[str, int]`, required.\n",
    "            A map from the word type to the index of the word.\n",
    "        vocab_size : `int`, required.\n",
    "            Size of the vocabulary.\n",
    "        word_embedding_size : `int`, required.\n",
    "            Size of word embeddings.\n",
    "        hidden_size : `int`, required.\n",
    "            Size of hidden layer or number of hidden units per layer.\n",
    "        num_intermediate_layers : `int`, required.\n",
    "            Number of intermediate layers, the arg takes 0 or greater integers.\n",
    "        dropout_rate : `float`, required.\n",
    "            Dropout rate.\n",
    "        use_glove : `bool`, optional.\n",
    "            Whether or not to use Glove embeddings instead of randomly initialized ones.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Return zero vector for input with padding_idx (0)\n",
    "        self.embedding = nn.Embedding(vocab_size, word_embedding_size)\n",
    "\n",
    "        # Hw-TODO: Add the intermediate layers, output layer, dropout layer,\n",
    "        #          and activation function according to DAN.\n",
    "        #          You may find [nn.Modulelist](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html)\n",
    "        #          useful to have multiple intermediate layers.\n",
    "        if num_intermediate_layers == 0:\n",
    "            self.hidden_layers = None\n",
    "            self.output_layer = nn.Linear(word_embedding_size, 1)\n",
    "        else:\n",
    "            self.hidden_layers = nn.ModuleList(\n",
    "                [nn.Linear(word_embedding_size, hidden_size)] + [\n",
    "                    nn.Linear(hidden_size, hidden_size)\n",
    "                    for _ in range(num_intermediate_layers - 1)\n",
    "                ])\n",
    "            self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_ids, lengths):\n",
    "        \"\"\"\n",
    "        # Parameters\n",
    "        input_ids : `torch.Tensor`, required.\n",
    "            Tensor of shape (batch_size, feature_length).\n",
    "            Each row is a datapoint represented by input words.\n",
    "        lengths: `torch.Tensor`, required.\n",
    "            Tensor of shape (batch_size, 1). Token length of input text.\n",
    "            Used to compute average word embeddings.\n",
    "        # Returns\n",
    "        probs : `torch.Tensor`\n",
    "            Tensor of shape (batch_size)\n",
    "        \"\"\"\n",
    "        print('input ids shape:',input_ids.shape)\n",
    "        out = self.embedding(input_ids)  # shape: (batch_sz, max_len, embedding_sz)\n",
    "        \n",
    "        # Hw-TODO: Use the intermediate layers, output layer, dropout layer,\n",
    "        #          and activation function you created in __init__\n",
    "        #          and other appropriate non-linearity for the output layer\n",
    "        #          to compute the probabilies of a class, assign these probabilities\n",
    "        #          to a variable named \"probs\".\n",
    " \n",
    "        out = torch.sum(input_ids, dim=1) / lengths  # shape: (batch_sz, embedding_sz)\n",
    "        if self.hidden_layers is not None:\n",
    "            for hidden_layer in self.hidden_layers:\n",
    "                out = hidden_layer(out)\n",
    "                out = self.activation(out)\n",
    "                out = self.dropout(out)\n",
    "        out = self.output_layer(out)\n",
    "        probs = torch.sigmoid(out)\n",
    "\n",
    "        return probs # you will define this variable in the preceding code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class SST2Dataset(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        #self.vocab = vocab\n",
    "        self.max_len = 512  # change when running full bioclinical bert\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        note = []\n",
    "        #label, text = int(self.data[index][0]), self.data[index][1]\n",
    "        #tokens = self.tokenizer.tokenize(text.lower())\n",
    "        # If word does not exist, give <unk> token id\n",
    "        #token_ids = [self.vocab.get(t, 1) for t in tokens]\n",
    "        length = self.max_len\n",
    "        features = sparse.load_npz(features_filepath).todense()\n",
    "        print('features shape:',features.shape)\n",
    "        #labels = np.load(labels_filepath, allow_pickle=True)[\"arr_0\"]\n",
    "        labels = np.asarray(sparse.load_npz(labels_filepath).todense()).ravel()\n",
    "        \n",
    "        # Truncate or pad to max length\n",
    "        #padded_token_ids = token_ids[:50] + [0] * (self.max_len - length)\n",
    "        return padded_token_ids, length, label\n",
    "\n",
    "    def collate_fn(self, batch_data):\n",
    "        padded_token_ids, lengths, labels = list(zip(*batch_data))\n",
    "        return (\n",
    "            torch.LongTensor(padded_token_ids).view(-1, self.max_len),\n",
    "            #torch.FloatTensor(lengths).view(-1, 1),\n",
    "            torch.FloatTensor(labels).view(-1, 1),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\"\"\"\n",
    "\n",
    "class DeepAveragingBinaryClassificationLModule(BinaryClassificationLModule):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def get_model(self) -> nn.Module:\n",
    "        return DeepAveragingNetworksModel(\n",
    "            #vocab=self.hparams.vocab,\n",
    "            vocab_size=self.hparams.train_batch_size,\n",
    "            word_embedding_size=self.hparams.word_embedding_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            num_intermediate_layers=self.hparams.num_intermediate_layers,\n",
    "            dropout_rate=self.hparams.dropout_rate,\n",
    "            use_glove=self.hparams.use_glove)\n",
    "\n",
    "    def batch2input(self, batch):\n",
    "        return {'input_ids': batch[0], 'lengths': 512}\n",
    "\n",
    "    def batch2labels(self, batch):\n",
    "        return batch[1].squeeze()\n",
    "\n",
    "    def get_dataloader(self, split, batch_size, shuffle=False) -> DataLoader:\n",
    "        data_dir = Path(self.hparams.data_dir)\n",
    "        features_filepath = data_dir.joinpath(\n",
    "            f\"{split}_{self.hparams.feature_name}_features.npz\")\n",
    "        labels_filepath = data_dir.joinpath(split + \"_labels.npz\")\n",
    "        features = sparse.load_npz(features_filepath).todense()\n",
    "        print('features shape:',features.shape)\n",
    "        #labels = np.load(labels_filepath, allow_pickle=True)[\"arr_0\"]\n",
    "        labels = np.asarray(sparse.load_npz(labels_filepath).todense()).ravel()\n",
    "        print('labels shape:',labels.shape)\n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(features).int(),\n",
    "            torch.from_numpy(labels).float())\n",
    "\n",
    "        logger.info(f\"Loading {split} data and labels from {labels_filepath}\")\n",
    "        data_loader = DataLoader(dataset=dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=shuffle,\n",
    "                                 num_workers=self.hparams.num_workers\n",
    "                                 #,collate_fn=dataset.collate_fn\n",
    "                                 )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.optimizer == 'sgd':\n",
    "            optimizer = torch.optim.SGD(self.model.parameters(),\n",
    "                                        lr=self.hparams.learning_rate)\n",
    "        elif self.hparams.optimizer == 'adam':\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                         lr=self.hparams.learning_rate)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # Hw-TODO: Add more optimizers and experiment with at least 2\n",
    "        #          optimizers other than vanilla SGD.\n",
    "        #          You can configure which optimizer to use by modifying\n",
    "        #          args_str or args passted to the function generic_train.\n",
    "        return optimizer\n",
    "\n",
    "    @classmethod\n",
    "    def add_model_specific_args(cls, parser: ArgumentParser) -> ArgumentParser:\n",
    "        parser = super().add_model_specific_args(parser)\n",
    "\n",
    "        # Required arguments\n",
    "        parser.add_argument('--num_intermediate_layers',\n",
    "                            type=int,\n",
    "                            help=\"number of intermediate layers\")\n",
    "        # Optional arguments\n",
    "        parser.add_argument('--dropout_rate',\n",
    "                            default=0.5,\n",
    "                            type=float,\n",
    "                            help=\"Dropout rate\")\n",
    "        parser.add_argument('--word_embedding_size',\n",
    "                            default=300,\n",
    "                            type=int,\n",
    "                            help=\"Size of word embeddings\")\n",
    "        parser.add_argument('--hidden_size',\n",
    "                            default=300,\n",
    "                            type=int,\n",
    "                            help=\"Size of hidden layer\")\n",
    "        parser.add_argument('--use_glove',\n",
    "                            action=\"store_true\",\n",
    "                            help=\"Whether to run predictions on the test set.\")\n",
    "        parser.add_argument('--task',\n",
    "                            default='danbinarycls',\n",
    "                            type=str,\n",
    "                            help=\"Name of the task.\")\n",
    "        parser.add_argument('--feature_name',\n",
    "                            default=None,\n",
    "                            type=str,\n",
    "                            required=True,\n",
    "                            help=\"Name of the feature\")\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1584: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name     | Type                       | Params\n",
      "--------------------------------------------------------\n",
      "0 | model    | DeepAveragingNetworksModel | 100 K \n",
      "1 | accuracy | Accuracy                   | 0     \n",
      "--------------------------------------------------------\n",
      "100 K     Trainable params\n",
      "0         Non-trainable params\n",
      "100 K     Total params\n",
      "0.401     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed arguments: Namespace(vocab_filename=None, optimizer='sgd', learning_rate=0.001, max_epochs=2, train_batch_size=32, eval_batch_size=32, seed=42, do_train=True, do_predict=True, data_dir='data', output_dir='output/dan', gpus=0, num_workers=8, num_intermediate_layers=1, dropout_rate=0.5, word_embedding_size=300, hidden_size=300, use_glove=False, task='danbinarycls', feature_name='roberta_readmit')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b25a0b06fd24f8bbf0b52b5c149da19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 19:12:05 - INFO - __main__ - Loading dev data and labels from data/dev_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (58328, 512)\n",
      "labels shape: (58328,)\n",
      "input ids shape: torch.Size([32, 512])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000014vscode-remote?line=23'>24</a>\u001b[0m Path(args\u001b[39m.\u001b[39moutput_dir)\u001b[39m.\u001b[39mmkdir(parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000014vscode-remote?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParsed arguments: \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000014vscode-remote?line=27'>28</a>\u001b[0m training_outout \u001b[39m=\u001b[39m generic_train(args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000014vscode-remote?line=28'>29</a>\u001b[0m                                 model_class\u001b[39m=\u001b[39;49mDeepAveragingBinaryClassificationLModule)\n",
      "\u001b[1;32m/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb Cell 4'\u001b[0m in \u001b[0;36mgeneric_train\u001b[0;34m(args, model_class)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=206'>207</a>\u001b[0m output_dict \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtrainer\u001b[39m\u001b[39m'\u001b[39m: trainer, \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m: model}\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=208'>209</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mdo_train:\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=209'>210</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit(model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=210'>211</a>\u001b[0m     \u001b[39m# Track model performance under differnt hparams settings in \"Hparams\" of TensorBoard\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=211'>212</a>\u001b[0m     tensorboard_logger\u001b[39m.\u001b[39mlog_hyperparams(\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=212'>213</a>\u001b[0m         params\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mhparams,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=213'>214</a>\u001b[0m         metrics\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mhp_metric\u001b[39m\u001b[39m'\u001b[39m: checkpoint_callback\u001b[39m.\u001b[39mbest_model_score\u001b[39m.\u001b[39mitem()})\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=734'>735</a>\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=735'>736</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=736'>737</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=737'>738</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=738'>739</a>\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=739'>740</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=740'>741</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=741'>742</a>\u001b[0m )\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=674'>675</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=675'>676</a>\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=676'>677</a>\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=681'>682</a>\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=682'>683</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=683'>684</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=684'>685</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=685'>686</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=686'>687</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=774'>775</a>\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=775'>776</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=776'>777</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=778'>779</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=779'>780</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1195'>1196</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1197'>1198</a>\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1198'>1199</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1200'>1201</a>\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1201'>1202</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1276'>1277</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1277'>1278</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1278'>1279</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=199'>200</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=200'>201</a>\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=201'>202</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_stage()\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1286'>1287</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1287'>1288</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1288'>1289</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1311\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1307'>1308</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_global_zero \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1308'>1309</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback\u001b[39m.\u001b[39mdisable()\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1310'>1311</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1312'>1313</a>\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1313'>1314</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1375\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1372'>1373</a>\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1373'>1374</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1374'>1375</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1376'>1377</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1378'>1379</a>\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:110\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=104'>105</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_fetcher \u001b[39m=\u001b[39m dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mget_profiled_dataloader(\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=105'>106</a>\u001b[0m     dataloader, dataloader_idx\u001b[39m=\u001b[39mdataloader_idx\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=106'>107</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=107'>108</a>\u001b[0m dl_max_batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_batches[dataloader_idx]\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=109'>110</a>\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(dataloader, dataloader_idx, dl_max_batches, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_dataloaders)\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=111'>112</a>\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=112'>113</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:122\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=119'>120</a>\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=120'>121</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mevaluation_step_and_end\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=121'>122</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=122'>123</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=124'>125</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:217\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=214'>215</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=215'>216</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=216'>217</a>\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mvalidation_step(step_kwargs)\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=218'>219</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:236\u001b[0m, in \u001b[0;36mAccelerator.validation_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=230'>231</a>\u001b[0m \u001b[39m\"\"\"The actual validation step.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=231'>232</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=232'>233</a>\u001b[0m \u001b[39mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.validation_step` for more details\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=233'>234</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=234'>235</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=235'>236</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:219\u001b[0m, in \u001b[0;36mTrainingTypePlugin.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=217'>218</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=218'>219</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb Cell 4'\u001b[0m in \u001b[0;36mBinaryClassificationLModule.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=35'>36</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch2input(batch)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=36'>37</a>\u001b[0m labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch2labels(batch)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=37'>38</a>\u001b[0m probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=38'>39</a>\u001b[0m probs \u001b[39m=\u001b[39m probs\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=40'>41</a>\u001b[0m \u001b[39m# Hw-TODO: Given probs in shape (batch_size,)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=41'>42</a>\u001b[0m \u001b[39m#          and labels of the same shape,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=42'>43</a>\u001b[0m \u001b[39m#          compute the binary cross entropy loss.\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb Cell 4'\u001b[0m in \u001b[0;36mBinaryClassificationLModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000004vscode-remote?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb Cell 10'\u001b[0m in \u001b[0;36mDeepAveragingNetworksModel.forward\u001b[0;34m(self, input_ids, lengths)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000010vscode-remote?line=49'>50</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000010vscode-remote?line=50'>51</a>\u001b[0m \u001b[39m# Parameters\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000010vscode-remote?line=51'>52</a>\u001b[0m \u001b[39minput_ids : `torch.Tensor`, required.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000010vscode-remote?line=59'>60</a>\u001b[0m \u001b[39m    Tensor of shape (batch_size)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000010vscode-remote?line=60'>61</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000010vscode-remote?line=61'>62</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minput ids shape:\u001b[39m\u001b[39m'\u001b[39m,input_ids\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000010vscode-remote?line=62'>63</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(input_ids)  \u001b[39m# shape: (batch_sz, max_len, embedding_sz)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000010vscode-remote?line=64'>65</a>\u001b[0m \u001b[39m# Hw-TODO: Use the intermediate layers, output layer, dropout layer,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000010vscode-remote?line=65'>66</a>\u001b[0m \u001b[39m#          and activation function you created in __init__\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000010vscode-remote?line=66'>67</a>\u001b[0m \u001b[39m#          and other appropriate non-linearity for the output layer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000010vscode-remote?line=67'>68</a>\u001b[0m \u001b[39m#          to compute the probabilies of a class, assign these probabilities\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000010vscode-remote?line=68'>69</a>\u001b[0m \u001b[39m#          to a variable named \"probs\".\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/nlp/n2c2-track2-nlp-uchicago/pytorch_pred.ipynb#ch0000010vscode-remote?line=70'>71</a>\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(input_ids, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m lengths  \u001b[39m# shape: (batch_sz, embedding_sz)\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=156'>157</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=157'>158</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=158'>159</a>\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=159'>160</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/torch/nn/functional.py:2044\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/functional.py?line=2037'>2038</a>\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/functional.py?line=2038'>2039</a>\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/functional.py?line=2039'>2040</a>\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/functional.py?line=2040'>2041</a>\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/functional.py?line=2041'>2042</a>\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/functional.py?line=2042'>2043</a>\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/torch/nn/functional.py?line=2043'>2044</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load hyperparameters\n",
    "parser = ArgumentParser()\n",
    "parser = DeepAveragingBinaryClassificationLModule.add_model_specific_args(parser)\n",
    "\n",
    "# NOTE: You should replace --optimizer <optimizer> with the name of the optimizer\n",
    "# with which you are experimenting with, and the same goes for word_embedding_size.\n",
    "# You can also configure other options listed in the method of add_model_specific_args of\n",
    "# the pytorch-lightning module `DeepAveragingBinaryClassificationLModule`.\n",
    "args_str = (\"--max_epochs 2 \"\n",
    "            \"--optimizer sgd --num_intermediate_layers 1 --feature_name roberta_readmit \"\n",
    "            \"--output_dir output/dan  --do_train --do_predict --gpus 0\")\n",
    "args = parser.parse_args(args_str.split())\n",
    "\n",
    "# If output_dir not provided, a folder is generated\n",
    "if args.output_dir is None:\n",
    "    args.output_dir = str(\n",
    "        Path('output').joinpath(\n",
    "            f\"{args.task}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"))\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Parsed arguments: {args}\")\n",
    "\n",
    "training_outout = generic_train(args=args,\n",
    "                                model_class=DeepAveragingBinaryClassificationLModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[stackoverflow: python - Adding L1/L2 regularization in PyTorch?](https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40f6b8985ae3d3af9736205d555f7ff87522357a9f5bdb6e88eda9160976b228"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
