{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion, Summary Exploration, and Processing of Notes Events records from MIMIC-III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,>=0.11.1\n",
      "  Downloading tokenizers-0.11.6-cp39-cp39-macosx_10_11_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: filelock in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: click in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /Users/gabemorrison/opt/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.4.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import transformers as ppb\n",
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "notes_file = DATA_DIR + '/NOTEEVENTS.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_notes = pd.read_csv(notes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_notes.columns)\n",
    "full_notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine single note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in first note: 889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ate:  [**2151-8-4**]\\n\\n\\nService:\\nADDENDUM:\\n\\nRADIOLOGIC STUDIES:  Radiologic studies also included a chest\\nCT, which confirmed cavitary lesions in the left lung apex\\nconsistent with infectious process/tuberculosis.  This also\\nmoderate-sized left pleural effusion.\\n\\nHEAD CT:  Head CT showed no intracranial hemorrhage or mass\\neffect, but old infarction consistent with past medical\\nhistory.\\n\\nABDOMINAL CT:  Abdominal CT showed lesions of\\nT10 and sacrum most likely secondary to osteoporosis. These can\\nbe followed by repeat imaging as an outpatient.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Characters in first note:',len(full_notes['TEXT'][0]))\n",
    "\n",
    "# partially truncating to avoid sharing full note due to confidentiality concerns\n",
    "full_notes['TEXT'][0][50:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RoBERTa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For RoBERTA:\n",
    "from transformers import RobertaConfig, RobertaModel\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "#Tokenizer:\n",
    "tokenizer_r = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "# Initializing a RoBERTa configuration\n",
    "configuration = RobertaConfig()\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model_r = RobertaModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_r = full_notes['TEXT'].apply((lambda x: tokenizer_r.encode(x, truncation=True, add_special_tokens=True)))\n",
    "max_len = 0\n",
    "for i in tokenized_r.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded_r = np.array([i + [0]*(max_len-len(i)) for i in tokenized_r.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2083180, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(padded_r.shape)\n",
    "padded_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crashed kernel\n",
    "sparse_padded_r = sparse.csr_matrix(padded_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_r_file = DATA_DIR + '/sparse_padded_r'\n",
    "sparse.save_npz(padded_r_file, sparse_padded_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BioClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = ppb.AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = ppb.AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioClinicalBERT tokenized shape: (2083180,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [101, 10296, 2236, 131, 164, 115, 115, 18615, ...\n",
       "1    [101, 10296, 2236, 131, 164, 115, 115, 20915, ...\n",
       "2    [101, 10296, 2236, 131, 164, 115, 115, 20915, ...\n",
       "3    [101, 10296, 2236, 131, 164, 115, 115, 19538, ...\n",
       "4    [101, 10296, 2236, 131, 164, 115, 115, 22148, ...\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize: \n",
    "tokenized_bc = full_notes['TEXT'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "print('BioClinicalBERT tokenized shape:',tokenized_bc.shape)\n",
    "tokenized_bc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioClinicalBERT tokenized shape: (2083180,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [101, 10296, 2236, 131, 164, 115, 115, 18615, ...\n",
       "1    [101, 10296, 2236, 131, 164, 115, 115, 20915, ...\n",
       "2    [101, 10296, 2236, 131, 164, 115, 115, 20915, ...\n",
       "3    [101, 10296, 2236, 131, 164, 115, 115, 19538, ...\n",
       "4    [101, 10296, 2236, 131, 164, 115, 115, 22148, ...\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize and pad: \n",
    "tokenized_bc = full_notes['TEXT'].apply((lambda x: tokenizer.encode(x, truncation=True, max_length=512, \\\n",
    "    padding=\"max_length\", add_special_tokens=True)))\n",
    "\n",
    "print('BioClinicalBERT tokenized shape:',tokenized_bc.shape)\n",
    "tokenized_bc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_bc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# didn't work\n",
    "tokenized_bc_np = np.array(tokenized_bc, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# didn't work\n",
    "sparse_bc = sparse.csr_matrix(np.array(tokenized_bc, dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "\n",
    "padded_bc_ar = asarray(padded_bc)\n",
    "padded_r_file = DATA_DIR + '/padded_bc'\n",
    "savetxt(padded_r_file, padded_bc_ar, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK to tokenize and manually produce vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2083180\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "counter = Counter()\n",
    "counter.update(['<pad>', '<unk>'])\n",
    "notes_text_list = list(full_notes['TEXT'])\n",
    "print(f\"Number of documents: {len(notes_text_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size after frequency filtering: 552755\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(notes_text_list):\n",
    "    tokens = tokenizer.tokenize(line)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    counter.update(tokens)\n",
    "    #if i < 3:\n",
    "    #    print(f\"String of line {i}: {line.strip()}\")\n",
    "    #    print(f\"Tokens of line {i}: {tokens}\")\n",
    "counter = dict(counter)\n",
    "\n",
    "#print(f\"Vocab size before frequency filtering: {len(counter)}\")\n",
    "\n",
    "vocab = {}\n",
    "for word, freq in counter.items():\n",
    "    #if freq < 3 and word not in ['<pad>', '<unk>']:\n",
    "    #    continue\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "print(f\"Vocab size after frequency filtering: {len(vocab)}\")\n",
    "output_filepath = DATA_DIR + '/unigram_vocab.json'\n",
    "json.dump(vocab, open(output_filepath, mode='w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2083180\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents:',full_notes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 753498364\n"
     ]
    }
   ],
   "source": [
    "print('Number of tokens:',sum(counter.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code for manually tokenizing; so far has been killing kernel so moved on to nltk above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Admission',\n",
       " 'Date:',\n",
       " '[**2151-7-16**]',\n",
       " 'Discharge',\n",
       " 'Date:',\n",
       " '[**2151-8-4**]',\n",
       " 'Service:',\n",
       " 'ADDENDUM:',\n",
       " 'RADIOLOGIC',\n",
       " 'STUDIES:',\n",
       " 'Radiologic',\n",
       " 'studies',\n",
       " 'also',\n",
       " 'included',\n",
       " 'a',\n",
       " 'chest',\n",
       " 'CT,',\n",
       " 'which',\n",
       " 'confirmed',\n",
       " 'cavitary',\n",
       " 'lesions',\n",
       " 'in',\n",
       " 'the',\n",
       " 'left',\n",
       " 'lung',\n",
       " 'apex',\n",
       " 'consistent',\n",
       " 'with',\n",
       " 'infectious',\n",
       " 'process/tuberculosis.',\n",
       " 'This',\n",
       " 'also',\n",
       " 'moderate-sized',\n",
       " 'left',\n",
       " 'pleural',\n",
       " 'effusion.',\n",
       " 'HEAD',\n",
       " 'CT:',\n",
       " 'Head',\n",
       " 'CT',\n",
       " 'showed',\n",
       " 'no',\n",
       " 'intracranial',\n",
       " 'hemorrhage',\n",
       " 'or',\n",
       " 'mass',\n",
       " 'effect,',\n",
       " 'but',\n",
       " 'old',\n",
       " 'infarction',\n",
       " 'consistent',\n",
       " 'with',\n",
       " 'past',\n",
       " 'medical',\n",
       " 'history.',\n",
       " 'ABDOMINAL',\n",
       " 'CT:',\n",
       " 'Abdominal',\n",
       " 'CT',\n",
       " 'showed',\n",
       " 'lesions',\n",
       " 'of',\n",
       " 'T10',\n",
       " 'and',\n",
       " 'sacrum',\n",
       " 'most',\n",
       " 'likely',\n",
       " 'secondary',\n",
       " 'to',\n",
       " 'osteoporosis.',\n",
       " 'These',\n",
       " 'can',\n",
       " 'be',\n",
       " 'followed',\n",
       " 'by',\n",
       " 'repeat',\n",
       " 'imaging',\n",
       " 'as',\n",
       " 'an',\n",
       " 'outpatient.',\n",
       " '[**First',\n",
       " 'Name8',\n",
       " '(NamePattern2)',\n",
       " '**]',\n",
       " '[**First',\n",
       " 'Name4',\n",
       " '(NamePattern1)',\n",
       " '1775**]',\n",
       " '[**Last',\n",
       " 'Name',\n",
       " '(NamePattern1)',\n",
       " '**],',\n",
       " 'M.D.',\n",
       " '[**MD',\n",
       " 'Number(1)',\n",
       " '1776**]',\n",
       " 'Dictated',\n",
       " 'By:[**Hospital',\n",
       " '1807**]',\n",
       " 'MEDQUIST36',\n",
       " 'D:',\n",
       " '[**2151-8-5**]',\n",
       " '12:11',\n",
       " 'T:',\n",
       " '[**2151-8-5**]',\n",
       " '12:21',\n",
       " 'JOB#:',\n",
       " '[**Job',\n",
       " 'Number',\n",
       " '1808**]',\n",
       " '']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split on both spaces and the \"\\n\" character for more accurate word count\n",
    "re.split(\"\\s+|\\n\",full_notes['TEXT'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re.split(\"\\s+|\\n\",full_notes['TEXT'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test splitting into words on small sample (killed kernel after 12 min on first attempt to run the full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_notes = full_notes.sample(n = 200, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>WORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>956690</th>\n",
       "      <td>961313</td>\n",
       "      <td>14734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2127-05-21</td>\n",
       "      <td>2127-05-21 09:01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>ABDOMEN (SUPINE ONLY)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2127-5-21**] 9:01 AM\\n ABDOMEN (SUPINE ONLY...</td>\n",
       "      <td>[[**2127-5-21**], 9:01, AM, ABDOMEN, (SUPINE, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712826</th>\n",
       "      <td>1713696</td>\n",
       "      <td>2576</td>\n",
       "      <td>110661.0</td>\n",
       "      <td>2169-11-15</td>\n",
       "      <td>2169-11-15 19:49:00</td>\n",
       "      <td>2169-11-15 19:50:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>21232.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Agree with above note by [**Initials (NamePatt...</td>\n",
       "      <td>[Agree, with, above, note, by, [**Initials, (N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033727</th>\n",
       "      <td>1043826</td>\n",
       "      <td>63616</td>\n",
       "      <td>175657.0</td>\n",
       "      <td>2113-11-21</td>\n",
       "      <td>2113-11-21 08:28:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PA &amp; LAT)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**Last Name (LF) **],[**First Name3 (LF) **] ...</td>\n",
       "      <td>[[**Last, Name, (LF), **],[**First, Name3, (LF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004537</th>\n",
       "      <td>1007150</td>\n",
       "      <td>28502</td>\n",
       "      <td>183617.0</td>\n",
       "      <td>2199-02-26</td>\n",
       "      <td>2199-02-26 03:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2199-2-26**] 3:15 AM\\n CHEST (PORTABLE AP) ...</td>\n",
       "      <td>[[**2199-2-26**], 3:15, AM, CHEST, (PORTABLE, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044022</th>\n",
       "      <td>1055606</td>\n",
       "      <td>96629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2118-02-17</td>\n",
       "      <td>2118-02-17 11:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CT C-SPINE W/O CONTRAST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2118-2-17**] 11:00 AM\\n CT C-SPINE W/O CONT...</td>\n",
       "      <td>[[**2118-2-17**], 11:00, AM, CT, C-SPINE, W/O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE            CHARTTIME  \\\n",
       "956690    961313       14734       NaN  2127-05-21  2127-05-21 09:01:00   \n",
       "1712826  1713696        2576  110661.0  2169-11-15  2169-11-15 19:49:00   \n",
       "1033727  1043826       63616  175657.0  2113-11-21  2113-11-21 08:28:00   \n",
       "1004537  1007150       28502  183617.0  2199-02-26  2199-02-26 03:15:00   \n",
       "1044022  1055606       96629       NaN  2118-02-17  2118-02-17 11:00:00   \n",
       "\n",
       "                   STORETIME       CATEGORY              DESCRIPTION     CGID  \\\n",
       "956690                   NaN      Radiology    ABDOMEN (SUPINE ONLY)      NaN   \n",
       "1712826  2169-11-15 19:50:00  Nursing/other                   Report  21232.0   \n",
       "1033727                  NaN      Radiology         CHEST (PA & LAT)      NaN   \n",
       "1004537                  NaN      Radiology      CHEST (PORTABLE AP)      NaN   \n",
       "1044022                  NaN      Radiology  CT C-SPINE W/O CONTRAST      NaN   \n",
       "\n",
       "         ISERROR                                               TEXT  \\\n",
       "956690       NaN  [**2127-5-21**] 9:01 AM\\n ABDOMEN (SUPINE ONLY...   \n",
       "1712826      NaN  Agree with above note by [**Initials (NamePatt...   \n",
       "1033727      NaN  [**Last Name (LF) **],[**First Name3 (LF) **] ...   \n",
       "1004537      NaN  [**2199-2-26**] 3:15 AM\\n CHEST (PORTABLE AP) ...   \n",
       "1044022      NaN  [**2118-2-17**] 11:00 AM\\n CT C-SPINE W/O CONT...   \n",
       "\n",
       "                                                     WORDS  \n",
       "956690   [[**2127-5-21**], 9:01, AM, ABDOMEN, (SUPINE, ...  \n",
       "1712826  [Agree, with, above, note, by, [**Initials, (N...  \n",
       "1033727  [[**Last, Name, (LF), **],[**First, Name3, (LF...  \n",
       "1004537  [[**2199-2-26**], 3:15, AM, CHEST, (PORTABLE, ...  \n",
       "1044022  [[**2118-2-17**], 11:00, AM, CT, C-SPINE, W/O,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_notes['WORDS'] = sample_notes['TEXT'].str.split(\"\\s+|\\n\")\n",
    "sample_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split size for easier processing: 694393\n",
      "notes shapes: (694393, 11) (694393, 11) (347196, 11) (347198, 11)\n"
     ]
    }
   ],
   "source": [
    "n_rows = full_notes.shape[0]\n",
    "split_size = round(n_rows/3)\n",
    "print('split size for easier processing:', split_size)\n",
    "\n",
    "notes_1 = full_notes.iloc[:split_size]\n",
    "notes_2 = full_notes.iloc[split_size:split_size*2]\n",
    "# splitting the last third into 2 due to some extremely large notes in final third (killed kernel mutliple times)\n",
    "notes_3 = full_notes.iloc[split_size*2:round(split_size*2.5)]\n",
    "notes_4 = full_notes.iloc[round(split_size*2.5):]\n",
    "print('notes shapes:',notes_1.shape, notes_2.shape, notes_3.shape, notes_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22105/3614905419.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes_1['WORDS'] = notes_1['TEXT'].str.split(\"\\s+|\\n\")\n"
     ]
    }
   ],
   "source": [
    "# takes approx 1.5 min to run\n",
    "notes_1['WORDS'] = notes_1['TEXT'].str.split(\"\\s+|\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_notes_toks = pd.concat([notes_1, notes_2, notes_3, notes_4])\n",
    "full_notes_toks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Analytics Vidhya: Fine-tune a RoBERTa Encoder-Decoder model trained on MLM for Text Generation](https://medium.com/analytics-vidhya/fine-tune-a-roberta-encoder-decoder-model-trained-on-mlm-for-text-generation-23da5f3c1858)  \n",
    "[StackOverflow: nlp - How to use Bert for long text classification?](https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40f6b8985ae3d3af9736205d555f7ff87522357a9f5bdb6e88eda9160976b228"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
