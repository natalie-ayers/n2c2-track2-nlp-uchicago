{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion, Summary Exploration, and Processing of Notes Events records from MIMIC-III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import transformers as ppb\n",
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "notes_file = DATA_DIR + '/NOTEEVENTS.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/npodpx/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "full_notes = pd.read_csv(notes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME',\n",
      "       'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'TEXT'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>2151-08-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>2118-06-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118.0</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>13702</td>\n",
       "      <td>196489.0</td>\n",
       "      <td>2124-08-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>26880</td>\n",
       "      <td>135453.0</td>\n",
       "      <td>2162-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE CHARTTIME STORETIME  \\\n",
       "0     174       22532  167853.0  2151-08-04       NaN       NaN   \n",
       "1     175       13702  107527.0  2118-06-14       NaN       NaN   \n",
       "2     176       13702  167118.0  2119-05-25       NaN       NaN   \n",
       "3     177       13702  196489.0  2124-08-18       NaN       NaN   \n",
       "4     178       26880  135453.0  2162-03-25       NaN       NaN   \n",
       "\n",
       "            CATEGORY DESCRIPTION  CGID  ISERROR  \\\n",
       "0  Discharge summary      Report   NaN      NaN   \n",
       "1  Discharge summary      Report   NaN      NaN   \n",
       "2  Discharge summary      Report   NaN      NaN   \n",
       "3  Discharge summary      Report   NaN      NaN   \n",
       "4  Discharge summary      Report   NaN      NaN   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(full_notes.columns)\n",
    "full_notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine single note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in first note: 889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ate:  [**2151-8-4**]\\n\\n\\nService:\\nADDENDUM:\\n\\nRADIOLOGIC STUDIES:  Radiologic studies also included a chest\\nCT, which confirmed cavitary lesions in the left lung apex\\nconsistent with infectious process/tuberculosis.  This also\\nmoderate-sized left pleural effusion.\\n\\nHEAD CT:  Head CT showed no intracranial hemorrhage or mass\\neffect, but old infarction consistent with past medical\\nhistory.\\n\\nABDOMINAL CT:  Abdominal CT showed lesions of\\nT10 and sacrum most likely secondary to osteoporosis. These can\\nbe followed by repeat imaging as an outpatient.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Characters in first note:',len(full_notes['TEXT'][0]))\n",
    "\n",
    "# partially truncating to avoid sharing full note due to confidentiality concerns\n",
    "full_notes['TEXT'][0][50:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RoBERTa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For RoBERTA:\n",
    "from transformers import RobertaConfig, RobertaModel\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "#Tokenizer:\n",
    "tokenizer_r = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "# Initializing a RoBERTa configuration\n",
    "configuration = RobertaConfig()\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model_r = RobertaModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_r = full_notes['TEXT'].apply((lambda x: tokenizer_r.encode(x, truncation=True, add_special_tokens=True)))\n",
    "max_len = 0\n",
    "for i in tokenized_r.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded_r = np.array([i + [0]*(max_len-len(i)) for i in tokenized_r.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2083180, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(padded_r.shape)\n",
    "padded_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crashed kernel\n",
    "sparse_padded_r = sparse.csr_matrix(padded_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_r_file = DATA_DIR + '/sparse_padded_r'\n",
    "sparse.save_npz(padded_r_file, sparse_padded_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BioClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = ppb.AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = ppb.AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioClinicalBERT tokenized shape: (2083180,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [101, 10296, 2236, 131, 164, 115, 115, 18615, ...\n",
       "1    [101, 10296, 2236, 131, 164, 115, 115, 20915, ...\n",
       "2    [101, 10296, 2236, 131, 164, 115, 115, 20915, ...\n",
       "3    [101, 10296, 2236, 131, 164, 115, 115, 19538, ...\n",
       "4    [101, 10296, 2236, 131, 164, 115, 115, 22148, ...\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize: \n",
    "tokenized_bc = full_notes['TEXT'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "print('BioClinicalBERT tokenized shape:',tokenized_bc.shape)\n",
    "tokenized_bc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize: \n",
    "tokenized_bc = full_notes['TEXT'].apply((lambda x: tokenizer.encode(x, truncation=True, max_length=512, add_special_tokens=True)))\n",
    "\n",
    "#Create Padding:\n",
    "max_len = 0\n",
    "for i in tokenized_bc.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded_bc = np.array([i + [0]*(max_len-len(i)) for i in tokenized_bc.values])\n",
    "#How it works: tokenized.values gives an array of lists of ints of various lengths\n",
    "#This goes through and says from the length of the list to max_len, fill in with 0\n",
    "print('BioClinicalBERT shape:',padded_bc.shape)\n",
    "padded_bc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "\n",
    "padded_bc_ar = asarray(padded_bc)\n",
    "padded_r_file = DATA_DIR + '/padded_bc'\n",
    "savetxt(padded_r_file, padded_bc_ar, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK to tokenize and manually produce vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2083180\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "counter = Counter()\n",
    "counter.update(['<pad>', '<unk>'])\n",
    "notes_text_list = list(full_notes['TEXT'])\n",
    "print(f\"Number of documents: {len(notes_text_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size after frequency filtering: 552755\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(notes_text_list):\n",
    "    tokens = tokenizer.tokenize(line)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    counter.update(tokens)\n",
    "    #if i < 3:\n",
    "    #    print(f\"String of line {i}: {line.strip()}\")\n",
    "    #    print(f\"Tokens of line {i}: {tokens}\")\n",
    "counter = dict(counter)\n",
    "\n",
    "#print(f\"Vocab size before frequency filtering: {len(counter)}\")\n",
    "\n",
    "vocab = {}\n",
    "for word, freq in counter.items():\n",
    "    #if freq < 3 and word not in ['<pad>', '<unk>']:\n",
    "    #    continue\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "print(f\"Vocab size after frequency filtering: {len(vocab)}\")\n",
    "output_filepath = DATA_DIR + '/unigram_vocab.json'\n",
    "json.dump(vocab, open(output_filepath, mode='w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2083180\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents:',full_notes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 753498364\n"
     ]
    }
   ],
   "source": [
    "print('Number of tokens:',sum(counter.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code for manually tokenizing; so far has been killing kernel so moved on to nltk above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Admission',\n",
       " 'Date:',\n",
       " '[**2151-7-16**]',\n",
       " 'Discharge',\n",
       " 'Date:',\n",
       " '[**2151-8-4**]',\n",
       " 'Service:',\n",
       " 'ADDENDUM:',\n",
       " 'RADIOLOGIC',\n",
       " 'STUDIES:',\n",
       " 'Radiologic',\n",
       " 'studies',\n",
       " 'also',\n",
       " 'included',\n",
       " 'a',\n",
       " 'chest',\n",
       " 'CT,',\n",
       " 'which',\n",
       " 'confirmed',\n",
       " 'cavitary',\n",
       " 'lesions',\n",
       " 'in',\n",
       " 'the',\n",
       " 'left',\n",
       " 'lung',\n",
       " 'apex',\n",
       " 'consistent',\n",
       " 'with',\n",
       " 'infectious',\n",
       " 'process/tuberculosis.',\n",
       " 'This',\n",
       " 'also',\n",
       " 'moderate-sized',\n",
       " 'left',\n",
       " 'pleural',\n",
       " 'effusion.',\n",
       " 'HEAD',\n",
       " 'CT:',\n",
       " 'Head',\n",
       " 'CT',\n",
       " 'showed',\n",
       " 'no',\n",
       " 'intracranial',\n",
       " 'hemorrhage',\n",
       " 'or',\n",
       " 'mass',\n",
       " 'effect,',\n",
       " 'but',\n",
       " 'old',\n",
       " 'infarction',\n",
       " 'consistent',\n",
       " 'with',\n",
       " 'past',\n",
       " 'medical',\n",
       " 'history.',\n",
       " 'ABDOMINAL',\n",
       " 'CT:',\n",
       " 'Abdominal',\n",
       " 'CT',\n",
       " 'showed',\n",
       " 'lesions',\n",
       " 'of',\n",
       " 'T10',\n",
       " 'and',\n",
       " 'sacrum',\n",
       " 'most',\n",
       " 'likely',\n",
       " 'secondary',\n",
       " 'to',\n",
       " 'osteoporosis.',\n",
       " 'These',\n",
       " 'can',\n",
       " 'be',\n",
       " 'followed',\n",
       " 'by',\n",
       " 'repeat',\n",
       " 'imaging',\n",
       " 'as',\n",
       " 'an',\n",
       " 'outpatient.',\n",
       " '[**First',\n",
       " 'Name8',\n",
       " '(NamePattern2)',\n",
       " '**]',\n",
       " '[**First',\n",
       " 'Name4',\n",
       " '(NamePattern1)',\n",
       " '1775**]',\n",
       " '[**Last',\n",
       " 'Name',\n",
       " '(NamePattern1)',\n",
       " '**],',\n",
       " 'M.D.',\n",
       " '[**MD',\n",
       " 'Number(1)',\n",
       " '1776**]',\n",
       " 'Dictated',\n",
       " 'By:[**Hospital',\n",
       " '1807**]',\n",
       " 'MEDQUIST36',\n",
       " 'D:',\n",
       " '[**2151-8-5**]',\n",
       " '12:11',\n",
       " 'T:',\n",
       " '[**2151-8-5**]',\n",
       " '12:21',\n",
       " 'JOB#:',\n",
       " '[**Job',\n",
       " 'Number',\n",
       " '1808**]',\n",
       " '']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split on both spaces and the \"\\n\" character for more accurate word count\n",
    "re.split(\"\\s+|\\n\",full_notes['TEXT'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re.split(\"\\s+|\\n\",full_notes['TEXT'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test splitting into words on small sample (killed kernel after 12 min on first attempt to run the full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_notes = full_notes.sample(n = 200, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>WORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>956690</th>\n",
       "      <td>961313</td>\n",
       "      <td>14734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2127-05-21</td>\n",
       "      <td>2127-05-21 09:01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>ABDOMEN (SUPINE ONLY)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2127-5-21**] 9:01 AM\\n ABDOMEN (SUPINE ONLY...</td>\n",
       "      <td>[[**2127-5-21**], 9:01, AM, ABDOMEN, (SUPINE, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712826</th>\n",
       "      <td>1713696</td>\n",
       "      <td>2576</td>\n",
       "      <td>110661.0</td>\n",
       "      <td>2169-11-15</td>\n",
       "      <td>2169-11-15 19:49:00</td>\n",
       "      <td>2169-11-15 19:50:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>21232.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Agree with above note by [**Initials (NamePatt...</td>\n",
       "      <td>[Agree, with, above, note, by, [**Initials, (N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033727</th>\n",
       "      <td>1043826</td>\n",
       "      <td>63616</td>\n",
       "      <td>175657.0</td>\n",
       "      <td>2113-11-21</td>\n",
       "      <td>2113-11-21 08:28:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PA &amp; LAT)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**Last Name (LF) **],[**First Name3 (LF) **] ...</td>\n",
       "      <td>[[**Last, Name, (LF), **],[**First, Name3, (LF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004537</th>\n",
       "      <td>1007150</td>\n",
       "      <td>28502</td>\n",
       "      <td>183617.0</td>\n",
       "      <td>2199-02-26</td>\n",
       "      <td>2199-02-26 03:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2199-2-26**] 3:15 AM\\n CHEST (PORTABLE AP) ...</td>\n",
       "      <td>[[**2199-2-26**], 3:15, AM, CHEST, (PORTABLE, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044022</th>\n",
       "      <td>1055606</td>\n",
       "      <td>96629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2118-02-17</td>\n",
       "      <td>2118-02-17 11:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CT C-SPINE W/O CONTRAST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2118-2-17**] 11:00 AM\\n CT C-SPINE W/O CONT...</td>\n",
       "      <td>[[**2118-2-17**], 11:00, AM, CT, C-SPINE, W/O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE            CHARTTIME  \\\n",
       "956690    961313       14734       NaN  2127-05-21  2127-05-21 09:01:00   \n",
       "1712826  1713696        2576  110661.0  2169-11-15  2169-11-15 19:49:00   \n",
       "1033727  1043826       63616  175657.0  2113-11-21  2113-11-21 08:28:00   \n",
       "1004537  1007150       28502  183617.0  2199-02-26  2199-02-26 03:15:00   \n",
       "1044022  1055606       96629       NaN  2118-02-17  2118-02-17 11:00:00   \n",
       "\n",
       "                   STORETIME       CATEGORY              DESCRIPTION     CGID  \\\n",
       "956690                   NaN      Radiology    ABDOMEN (SUPINE ONLY)      NaN   \n",
       "1712826  2169-11-15 19:50:00  Nursing/other                   Report  21232.0   \n",
       "1033727                  NaN      Radiology         CHEST (PA & LAT)      NaN   \n",
       "1004537                  NaN      Radiology      CHEST (PORTABLE AP)      NaN   \n",
       "1044022                  NaN      Radiology  CT C-SPINE W/O CONTRAST      NaN   \n",
       "\n",
       "         ISERROR                                               TEXT  \\\n",
       "956690       NaN  [**2127-5-21**] 9:01 AM\\n ABDOMEN (SUPINE ONLY...   \n",
       "1712826      NaN  Agree with above note by [**Initials (NamePatt...   \n",
       "1033727      NaN  [**Last Name (LF) **],[**First Name3 (LF) **] ...   \n",
       "1004537      NaN  [**2199-2-26**] 3:15 AM\\n CHEST (PORTABLE AP) ...   \n",
       "1044022      NaN  [**2118-2-17**] 11:00 AM\\n CT C-SPINE W/O CONT...   \n",
       "\n",
       "                                                     WORDS  \n",
       "956690   [[**2127-5-21**], 9:01, AM, ABDOMEN, (SUPINE, ...  \n",
       "1712826  [Agree, with, above, note, by, [**Initials, (N...  \n",
       "1033727  [[**Last, Name, (LF), **],[**First, Name3, (LF...  \n",
       "1004537  [[**2199-2-26**], 3:15, AM, CHEST, (PORTABLE, ...  \n",
       "1044022  [[**2118-2-17**], 11:00, AM, CT, C-SPINE, W/O,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_notes['WORDS'] = sample_notes['TEXT'].str.split(\"\\s+|\\n\")\n",
    "sample_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split size for easier processing: 694393\n",
      "notes shapes: (694393, 11) (694393, 11) (347196, 11) (347198, 11)\n"
     ]
    }
   ],
   "source": [
    "n_rows = full_notes.shape[0]\n",
    "split_size = round(n_rows/3)\n",
    "print('split size for easier processing:', split_size)\n",
    "\n",
    "notes_1 = full_notes.iloc[:split_size]\n",
    "notes_2 = full_notes.iloc[split_size:split_size*2]\n",
    "# splitting the last third into 2 due to some extremely large notes in final third (killed kernel mutliple times)\n",
    "notes_3 = full_notes.iloc[split_size*2:round(split_size*2.5)]\n",
    "notes_4 = full_notes.iloc[round(split_size*2.5):]\n",
    "print('notes shapes:',notes_1.shape, notes_2.shape, notes_3.shape, notes_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22105/3614905419.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes_1['WORDS'] = notes_1['TEXT'].str.split(\"\\s+|\\n\")\n"
     ]
    }
   ],
   "source": [
    "# takes approx 1.5 min to run\n",
    "notes_1['WORDS'] = notes_1['TEXT'].str.split(\"\\s+|\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_notes_toks = pd.concat([notes_1, notes_2, notes_3, notes_4])\n",
    "full_notes_toks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Analytics Vidhya: Fine-tune a RoBERTa Encoder-Decoder model trained on MLM for Text Generation](https://medium.com/analytics-vidhya/fine-tune-a-roberta-encoder-decoder-model-trained-on-mlm-for-text-generation-23da5f3c1858)  \n",
    "[StackOverflow: nlp - How to use Bert for long text classification?](https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40f6b8985ae3d3af9736205d555f7ff87522357a9f5bdb6e88eda9160976b228"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
