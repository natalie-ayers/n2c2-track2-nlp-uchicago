{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression to predict stay length from MIMIC-III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple, Type\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchmetrics import MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, R2Score\n",
    "\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear regression model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        # Parameters\n",
    "        num_features : `int`, required.\n",
    "            Number of the features.\n",
    "        # Returns\n",
    "            `None`\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Hw-TODO: Add a linear layer to weight the features.\n",
    "        self.linear = nn.Linear(in_features=num_features, out_features=1, bias=True)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Returns the logits of the model given features. \n",
    "        Note that model predictions should be either 0 or 1 based on a threshold.\n",
    "        # Parameters\n",
    "        features : `torch.FloatTensor`, required.\n",
    "            The tensor of features with the shape (batch_size, num_of_features)\n",
    "        # Returns\n",
    "        probs : `torch.FloatTensor`, required.\n",
    "            The tensor of output values with the shape (batch_size, 1) or (batch_size,)\n",
    "        \"\"\"\n",
    "        # Hw-TODO: Use `self.linear` you created in `__init__`\n",
    "        #          and appropriate nonlinearity/activation-function to compute\n",
    "        #          and return the probabilities of belonging to a class in the logistic regression.\n",
    "        out = self.linear(features)\n",
    "        #out = self.dropout(out)\n",
    "        #probs = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save arguments to `hparams` attribute, see the doc [here](https://pytorch-lightning.readthedocs.io/en/latest/common/hyperparameters.html).\n",
    "        self.save_hyperparameters()\n",
    "        data_dir = Path(self.hparams.data_dir)\n",
    "        #self.hparams.vocab = json.load(\n",
    "        #    open(data_dir.joinpath(self.hparams.vocab_filename)))\n",
    "        #self.hparams.vocab_size = len(self.hparams.vocab)\n",
    "\n",
    "        self.model = self.get_model()\n",
    "        self.step_count = 0\n",
    "        #self.accuracy = Accuracy()\n",
    "        #self.pr = Precision(threshold=0.5,average='macro',num_classes=1,multiclass=False)\n",
    "        self.mse = MeanSquaredError()\n",
    "        self.mape = MeanAbsolutePercentageError()\n",
    "        self.mae = MeanAbsoluteError()\n",
    "        self.r2 = R2Score() \n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = self.batch2labels(batch)\n",
    "        probs = self(**input)\n",
    "        probs = probs.squeeze()\n",
    "        # Hw-TODO: Given probs in shape (batch_size,)\n",
    "        #          and labels of the same shape,\n",
    "        #          compute the binary cross entropy loss.\n",
    "        loss = nn.functional.mse_loss(probs, labels)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_mse', self.mse(probs, labels.int()), prog_bar=True)\n",
    "        self.log('train_mape', self.mape(probs, labels.int()))\n",
    "        self.log('train_mae', self.mae(probs, labels.int()))\n",
    "        self.log('train_r2', self.r2(probs, labels.int()))\n",
    "        output_dict = {'loss': loss}\n",
    "        return output_dict\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = self.batch2labels(batch)\n",
    "        probs = self(**input)\n",
    "        probs = probs.squeeze()\n",
    "\n",
    "        # Hw-TODO: Given probs in shape (batch_size,)\n",
    "        #          and labels of the same shape,\n",
    "        #          compute the binary cross entropy loss.\n",
    "        loss = nn.functional.mse_loss(probs, labels)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_mse', self.mse(probs, labels.int()), prog_bar=True)\n",
    "        self.log('val_mape', self.mape(probs, labels.int()))\n",
    "        self.log('val_mae', self.mae(probs, labels.int()))\n",
    "        self.log('val_r2', self.r2(probs, labels.int()))\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = self.batch2labels(batch)\n",
    "        probs = self(**input)\n",
    "        probs = probs.squeeze()\n",
    "\n",
    "        # Hw-TODO: Given probs in shape (batch_size,)\n",
    "        #          and labels of the same shape,\n",
    "        #          compute the binary cross entropy loss.\n",
    "        loss = nn.functional.mse_loss(probs, labels)\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_mse', self.mse(probs, labels.int()), prog_bar=True)\n",
    "        self.log('test_mape', self.mape(probs, labels.int()))\n",
    "        self.log('test_mae', self.mae(probs, labels.int()))\n",
    "        self.log('test_r2', self.r2(probs, labels.int()))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.optimizer == 'sgd':\n",
    "            optimizer = torch.optim.SGD(self.model.parameters(),\n",
    "                                        lr=self.hparams.learning_rate)\n",
    "        elif self.hparams.optimizer == 'adam':\n",
    "            if self.hparams.l2_regularization:\n",
    "                optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                         lr=self.hparams.learning_rate,\n",
    "                                         weight_decay=1e-5)\n",
    "            else:\n",
    "                optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                         lr=self.hparams.learning_rate)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader('train', self.hparams.train_batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader('dev', self.hparams.eval_batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.get_dataloader('test', self.hparams.eval_batch_size, shuffle=False)\n",
    "\n",
    "    def get_model(self) -> nn.Module:\n",
    "        # To be overridden by inherited classes.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def batch2input(self, batch: Tuple[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # To be overridden by inherited classes.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def batch2labels(self, batch: Tuple[torch.Tensor]) -> torch.Tensor:\n",
    "        # To be overridden by inherited classes.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_dataloader(self,\n",
    "                       split: str,\n",
    "                       batch_size: int,\n",
    "                       shuffle: bool = False) -> DataLoader:\n",
    "        # To be overridden by inherited classes.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def add_model_specific_args(cls, parser: ArgumentParser) -> ArgumentParser:\n",
    "        \"\"\"\n",
    "        Add arguments to the parser and return the parser.\n",
    "        \"\"\"\n",
    "        # Required arguments:\n",
    "        parser.add_argument('--vocab_filename',\n",
    "                            default=None,\n",
    "                            type=str,\n",
    "                            required=False,  # changing to false since we're not using vocab\n",
    "                            help=\"File name of the feature.\")\n",
    "        # Optional arguments:\n",
    "        parser.add_argument('--optimizer',\n",
    "                            default='adam',\n",
    "                            type=str,\n",
    "                            help=\"The optimizer to use, such as sgd or adam.\")\n",
    "        parser.add_argument('--learning_rate',\n",
    "                            default=1e-3,\n",
    "                            type=float,\n",
    "                            help=\"The initial learning rate for training.\")\n",
    "        parser.add_argument('--l2_regularization',\n",
    "                            default=False,\n",
    "                            help=\"Whether to add L2 regularization.\")\n",
    "        parser.add_argument('--max_epochs',\n",
    "                            default=10,\n",
    "                            type=int,\n",
    "                            help=\"The number of epochs to train your model.\")\n",
    "        parser.add_argument('--train_batch_size', default=32, type=int)\n",
    "        parser.add_argument('--eval_batch_size', default=32, type=int)\n",
    "        parser.add_argument('--seed',\n",
    "                            type=int,\n",
    "                            default=42,\n",
    "                            help=\"The random seed for initialization\")\n",
    "        parser.add_argument('--do_train',\n",
    "                            action=\"store_true\",\n",
    "                            default=True,\n",
    "                            help=\"Whether to run training.\")\n",
    "        parser.add_argument('--do_predict',\n",
    "                            action=\"store_true\",\n",
    "                            help=\"Whether to run predictions on the test set.\")\n",
    "        parser.add_argument('--data_dir',\n",
    "                            default=\"data\",\n",
    "                            type=str,\n",
    "                            help=\"The input data dir. Should contain the training files.\")\n",
    "        parser.add_argument('--output_dir',\n",
    "                            type=str,\n",
    "                            help=(\"The output directory where the model predictions \"\n",
    "                                  \"and checkpoints will be written.\"))\n",
    "        # NOTE: Set --gpus 0 or change the default value to 0 if not using GPUS.\n",
    "        # See this [link](https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu.html) for usage of this argument.\n",
    "        parser.add_argument('--gpus',\n",
    "                            default=1,\n",
    "                            type=int,\n",
    "                            help=\"The number of GPUs allocated for this, 0 meaning none\")\n",
    "        parser.add_argument('--num_workers',\n",
    "                            default=8,\n",
    "                            type=int,\n",
    "                            help=\"Config `DataLoader` of pytorch\")\n",
    "        return parser\n",
    "\n",
    "\n",
    "def generic_train(args: argparse.Namespace,\n",
    "                  model_class: Type[pl.LightningModule]) -> Dict:\n",
    "    \"\"\"\n",
    "        Train (and optionally predict) and return dict results.\n",
    "        # Parameters\n",
    "        args : `argparse.Namespace`, required.\n",
    "            Configuration of the training and the model\n",
    "        model_class : `Type[pl.LightningModule]`, required.\n",
    "            Class of the model to be trained.\n",
    "        # Returns\n",
    "        A `dict` object containing the following keys and types.\n",
    "            trainer: `pl.Trainer`\n",
    "            model: `pl.LightningModule`\n",
    "            val_results_best: `list[dict]`\n",
    "                If `args.do_predict==True`\n",
    "            test_results_best: `list[dict]`\n",
    "                If `args.do_predict==True`\n",
    "            best_model_path: `Path`\n",
    "                Path to the checkpoint of the best model.\n",
    "        \"\"\"\n",
    "    pl.seed_everything(args.seed)\n",
    "\n",
    "    tensorboard_log_dir = Path(args.output_dir).joinpath('tensorboard_logs')\n",
    "    tensorboard_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Tensorboard logger\n",
    "    tensorboard_logger = pl_loggers.TensorBoardLogger(\n",
    "        save_dir=tensorboard_log_dir,\n",
    "        version='version_' + datetime.now().strftime('%Y%m%d-%H%M%S'),\n",
    "        name='',\n",
    "        default_hp_metric=True)\n",
    "    # Checkpoint callback\n",
    "    checkpoint_dir = Path(args.output_dir).joinpath(tensorboard_logger.version,\n",
    "                                                    'checkpoints')\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=checkpoint_dir,\n",
    "                                                       filename='{epoch}-{val_mse:.2f}',\n",
    "                                                       monitor='val_mse',\n",
    "                                                       mode='min',   # minimize mse\n",
    "                                                       save_top_k=1,\n",
    "                                                       verbose=True)\n",
    "\n",
    "    dict_args = vars(args)\n",
    "    model = model_class(**dict_args)\n",
    "    trainer = pl.Trainer.from_argparse_args(args,\n",
    "                                            logger=tensorboard_logger,\n",
    "                                            callbacks=[checkpoint_callback])\n",
    "\n",
    "    output_dict = {'trainer': trainer, 'model': model}\n",
    "\n",
    "    if args.do_train:\n",
    "        trainer.fit(model=model)\n",
    "        # Track model performance under differnt hparams settings in \"Hparams\" of TensorBoard\n",
    "        tensorboard_logger.log_hyperparams(\n",
    "            params=model.hparams,\n",
    "            metrics={'hp_metric': checkpoint_callback.best_model_score.item()})\n",
    "        tensorboard_logger.save()\n",
    "\n",
    "        # Save the best model to `best_model.ckpt`\n",
    "        best_model_path = checkpoint_dir.joinpath('best_model.ckpt')\n",
    "        logger.info(f\"Copy best model from {checkpoint_callback.best_model_path} \"\n",
    "                    f\"to {best_model_path}.\")\n",
    "        shutil.copy(checkpoint_callback.best_model_path, best_model_path)\n",
    "\n",
    "        output_dict.update({\n",
    "            'trainer': trainer,\n",
    "            'model': model,\n",
    "            'best_model_path': best_model_path\n",
    "        })\n",
    "\n",
    "    # Optionally, predict on test set.\n",
    "    if args.do_predict:\n",
    "        best_model_path = checkpoint_dir.joinpath('best_model.ckpt')\n",
    "        model = model.load_from_checkpoint(best_model_path)\n",
    "        val_results_best = trainer.validate(model, verbose=True)\n",
    "        test_results_best = trainer.test(model, verbose=True)\n",
    "        print(\"Validation MSE on the best model: {: .4f}\".format(\n",
    "            val_results_best[0]['val_mse']))\n",
    "        print(\"Validation R2 on the best model: {: .4f}\".format(\n",
    "            val_results_best[0]['val_r2']))\n",
    "        #print('val_results_best all',val_results_best[0])\n",
    "        #print('val_results_best_trulyall',val_results_best)\n",
    "        print(\"Test MSE on the best model: {: .4f}\".format(\n",
    "            test_results_best[0]['test_mse']))\n",
    "        print(\"Test R2 on the best model: {: .4f}\".format(\n",
    "            test_results_best[0]['test_r2']))\n",
    "        output_dict.update({\n",
    "            'val_results_best': val_results_best,\n",
    "            'test_results_best': test_results_best,\n",
    "        })\n",
    "\n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureBasedRegressionModule(RegressionModule):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def get_model(self) -> nn.Module:\n",
    "        return LinearRegressionModel(num_features=764)\n",
    "\n",
    "    def batch2input(self, batch):\n",
    "        return {'features': batch[0]}\n",
    "\n",
    "    def batch2labels(self, batch):\n",
    "        return batch[1]\n",
    "\n",
    "    def get_dataloader(self,\n",
    "                       split: str,\n",
    "                       batch_size: int,\n",
    "                       shuffle: bool = False) -> DataLoader:\n",
    "        # NOTE: In order to use different features, change feature_name by\n",
    "        # passing `--feature_name <feature_name>` in the training loop in\n",
    "        # the cell below, or revise the code here for correct paths if needed.\n",
    "        data_dir = Path(self.hparams.data_dir)\n",
    "        features_filepath = data_dir.joinpath(\n",
    "            f\"{split}_{self.hparams.feature_name}_features.npz\")\n",
    "        labels_filepath = data_dir.joinpath(f\"{split}_{self.hparams.label_name}_labels.npz\")\n",
    "        features = sparse.load_npz(features_filepath).todense()\n",
    "        print('features shape:',features.shape)\n",
    "        #labels = np.load(labels_filepath, allow_pickle=True)[\"arr_0\"]\n",
    "        labels = np.asarray(sparse.load_npz(labels_filepath).todense()).ravel()\n",
    "        print('labels shape:',labels.shape)\n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(features).float(),\n",
    "            torch.from_numpy(labels).float())\n",
    "\n",
    "        logger.info(f\"Loading {split} features and labels \"\n",
    "                    f\"from {features_filepath} and {labels_filepath}\")\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=shuffle,\n",
    "                                                  num_workers=self.hparams.num_workers)\n",
    "        return data_loader\n",
    "\n",
    "    @classmethod\n",
    "    def add_model_specific_args(cls, parser: ArgumentParser) -> ArgumentParser:\n",
    "        parser = super().add_model_specific_args(parser)\n",
    "        # Required arguments:\n",
    "        parser.add_argument('--feature_name',\n",
    "                            default=None,\n",
    "                            type=str,\n",
    "                            required=True,\n",
    "                            help=\"Name of the feature\")\n",
    "                            \n",
    "        parser.add_argument('--label_name',\n",
    "                            default=None,\n",
    "                            type=str,\n",
    "                            required=True,\n",
    "                            help=\"Name of the label\")\n",
    "        # Optional arguments:\n",
    "        parser.add_argument('--task',\n",
    "                            default='featurebinarycls',\n",
    "                            type=str,\n",
    "                            help=\"Name of the task.\")\n",
    "        return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                        | Params\n",
      "------------------------------------------------------\n",
      "0 | model | LinearRegressionModel       | 765   \n",
      "1 | mse   | MeanSquaredError            | 0     \n",
      "2 | mape  | MeanAbsolutePercentageError | 0     \n",
      "3 | mae   | MeanAbsoluteError           | 0     \n",
      "4 | r2    | R2Score                     | 0     \n",
      "------------------------------------------------------\n",
      "765       Trainable params\n",
      "0         Non-trainable params\n",
      "765       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed arguments: Namespace(vocab_filename=None, optimizer='adam', learning_rate=0.001, l2_regularization=False, max_epochs=10, train_batch_size=32, eval_batch_size=32, seed=42, do_train=True, do_predict=True, data_dir='data', output_dir='output/sl_linear', gpus=1, num_workers=8, feature_name='roberta', label_name='stay_len', task='featurebinarycls')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30496f9f1791420892780f37553eb766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 01:21:06 - INFO - __main__ - Loading dev features and labels from data/dev_roberta_features.npz and data/dev_stay_len_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "2022-03-15 01:21:07 - INFO - __main__ - Loading train features and labels from data/train_roberta_features.npz and data/train_stay_len_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (34416, 764)\n",
      "labels shape: (34416,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb4f99055d547e8b33b25d272d13e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc24bd86a5f74016b073e9dcadc6b36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1075: val_mse reached 1174069968896.00000 (best 1174069968896.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-012105/checkpoints/epoch=0-val_mse=1174069968896.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7f3088548a40ad8293e9d2bc983bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 2151: val_mse reached 1159325155328.00000 (best 1159325155328.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-012105/checkpoints/epoch=1-val_mse=1159325155328.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377c24422b414d09b3a2f098aca5a49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 3227: val_mse reached 1148867182592.00000 (best 1148867182592.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-012105/checkpoints/epoch=2-val_mse=1148867182592.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d049a506b8f04db4b1ff81fcde3deedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 4303: val_mse reached 1142567206912.00000 (best 1142567206912.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-012105/checkpoints/epoch=3-val_mse=1142567206912.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba085f651c842a6841b949f54e644dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 5379: val_mse reached 1138437783552.00000 (best 1138437783552.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-012105/checkpoints/epoch=4-val_mse=1138437783552.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4330c93f66674e0bbf1006616815bd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 6455: val_mse reached 1135132016640.00000 (best 1135132016640.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-012105/checkpoints/epoch=5-val_mse=1135132016640.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccfc3a80a894ac69f79f7e43bfba2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 7531: val_mse reached 1132903268352.00000 (best 1132903268352.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-012105/checkpoints/epoch=6-val_mse=1132903268352.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c2a86dd8b44710872ca7ed74c0779a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 8607: val_mse reached 1131181768704.00000 (best 1131181768704.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-012105/checkpoints/epoch=7-val_mse=1131181768704.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efba99da4c9b4ff182ebae1ae7a31636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 9683: val_mse reached 1130150625280.00000 (best 1130150625280.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-012105/checkpoints/epoch=8-val_mse=1130150625280.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1049454b523400eac79a1524cb48b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 10759: val_mse reached 1128509734912.00000 (best 1128509734912.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-012105/checkpoints/epoch=9-val_mse=1128509734912.00.ckpt\" as top 1\n",
      "2022-03-15 01:23:18 - INFO - __main__ - Copy best model from /mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-012105/checkpoints/epoch=9-val_mse=1128509734912.00.ckpt to output/sl_linear/version_20220315-012105/checkpoints/best_model.ckpt.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2022-03-15 01:23:18 - INFO - __main__ - Loading dev features and labels from data/dev_roberta_features.npz and data/dev_stay_len_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a008b26c3e604c96933980df569c533d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_loss': 1128509734912.0,\n",
      " 'val_mae': 599885.6875,\n",
      " 'val_mape': 2.469804048538208,\n",
      " 'val_mse': 1128509734912.0,\n",
      " 'val_r2': 2.2260525226593018}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 01:23:21 - INFO - __main__ - Loading test features and labels from data/test_roberta_features.npz and data/test_stay_len_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5207d795bd87492bb12253613cda4d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 1065926262784.0,\n",
      " 'test_mae': 605325.5625,\n",
      " 'test_mape': 2.2779018878936768,\n",
      " 'test_mse': 1065926262784.0,\n",
      " 'test_r2': 2.16449236869812}\n",
      "--------------------------------------------------------------------------------\n",
      "Validation MSE on the best model:  1128509734912.0000\n",
      "Validation R2 on the best model:  2.2261\n",
      "Test MSE on the best model:  1065926262784.0000\n",
      "Test R2 on the best model:  2.1645\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load hyperparameters\n",
    "parser = ArgumentParser()\n",
    "parser = FeatureBasedRegressionModule.add_model_specific_args(parser)\n",
    "\n",
    "# NOTE: You should replace `unigram_binary` in the assignment statement of `args_str =...`\n",
    "# with whatever feature that you are experimented with.\n",
    "# You can also configure other options listed in the method of add_model_specific_args of\n",
    "# the pytorch-lightning model `FeatureBasedBinaryClassificationLModule`.\n",
    "args_str = (\"--feature_name roberta --max_epochs 10 --label_name stay_len \"\n",
    "            \"--output_dir output/sl_linear --optimizer adam --do_train --do_predict\")\n",
    "\n",
    "args = parser.parse_args(args_str.split())\n",
    "\n",
    "# If output_dir not provided, a folder is generated\n",
    "if args.output_dir is None:\n",
    "    args.output_dir = str(\n",
    "        Path('output').joinpath(\n",
    "            f\"{args.task}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"))\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Parsed arguments: {args}\")\n",
    "\n",
    "training_outout = generic_train(args=args,\n",
    "                                model_class=FeatureBasedRegressionModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                        | Params\n",
      "------------------------------------------------------\n",
      "0 | model | LinearRegressionModel       | 765   \n",
      "1 | mse   | MeanSquaredError            | 0     \n",
      "2 | mape  | MeanAbsolutePercentageError | 0     \n",
      "3 | mae   | MeanAbsoluteError           | 0     \n",
      "4 | r2    | R2Score                     | 0     \n",
      "------------------------------------------------------\n",
      "765       Trainable params\n",
      "0         Non-trainable params\n",
      "765       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed arguments: Namespace(vocab_filename=None, optimizer='adam', learning_rate=0.001, l2_regularization=False, max_epochs=20, train_batch_size=32, eval_batch_size=32, seed=42, do_train=True, do_predict=True, data_dir='data', output_dir='output/sl_linear', gpus=1, num_workers=8, feature_name='roberta', label_name='stay_len', task='featurebinarycls')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e389acb71e7d45929ab7e66533246144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 01:33:04 - INFO - __main__ - Loading dev features and labels from data/dev_roberta_features.npz and data/dev_stay_len_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "2022-03-15 01:33:05 - INFO - __main__ - Loading train features and labels from data/train_roberta_features.npz and data/train_stay_len_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (34416, 764)\n",
      "labels shape: (34416,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31209078e6624247a3aff2268a2ffdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416c69a0c226431fad4e857c3422a847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1075: val_mse reached 1174069968896.00000 (best 1174069968896.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=0-val_mse=1174069968896.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ed6c41b2b44d4bb872b4fd39a31e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 2151: val_mse reached 1159325155328.00000 (best 1159325155328.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=1-val_mse=1159325155328.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3402684d32420e871b176e5c2b9c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 3227: val_mse reached 1148867182592.00000 (best 1148867182592.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=2-val_mse=1148867182592.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cd88f649b54bc09f54ac09e3e444b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 4303: val_mse reached 1142567206912.00000 (best 1142567206912.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=3-val_mse=1142567206912.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18454668bdd049dd98090aeacc576648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 5379: val_mse reached 1138437783552.00000 (best 1138437783552.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=4-val_mse=1138437783552.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19836a733c1247d89aecd7022b323b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 6455: val_mse reached 1135132016640.00000 (best 1135132016640.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=5-val_mse=1135132016640.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4353d6080c064e7d82ef5dc77ccacf75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 7531: val_mse reached 1132903268352.00000 (best 1132903268352.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=6-val_mse=1132903268352.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561998f09032489d9872db82c9c97c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 8607: val_mse reached 1131181768704.00000 (best 1131181768704.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=7-val_mse=1131181768704.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466687d21e494b668ac64e5c09401830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 9683: val_mse reached 1130150625280.00000 (best 1130150625280.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=8-val_mse=1130150625280.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973c53d82f414c13b450f5d13517b6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 10759: val_mse reached 1128509734912.00000 (best 1128509734912.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=9-val_mse=1128509734912.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8c9f22aa524cde98fcb39a046d611a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 11835: val_mse reached 1128212594688.00000 (best 1128212594688.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=10-val_mse=1128212594688.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b023f202a7a84493ba1fa9d9060c4da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 12911: val_mse reached 1126949978112.00000 (best 1126949978112.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=11-val_mse=1126949978112.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccb0a14f14d42b283ea5337f8a90dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 13987: val_mse reached 1126548766720.00000 (best 1126548766720.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=12-val_mse=1126548766720.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ced701c48e1444ebaff7601b1aaa8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 15063: val_mse reached 1125977292800.00000 (best 1125977292800.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=13-val_mse=1125977292800.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c1ff2bd3e449fbbfed4be8f18fc62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 16139: val_mse was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078131244166496c803776c735c076df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 17215: val_mse reached 1125297029120.00000 (best 1125297029120.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=15-val_mse=1125297029120.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4f490fb87b404881fda8addcf7e2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 18291: val_mse reached 1125173952512.00000 (best 1125173952512.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=16-val_mse=1125173952512.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7e2270b3c3425e8f4fcab1f3a6d53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 19367: val_mse reached 1124833165312.00000 (best 1124833165312.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=17-val_mse=1124833165312.00.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fa92432f1a4b57afe176439b5385cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 20443: val_mse was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c71978a15854908a5e5744e581fa446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 21519: val_mse reached 1124531044352.00000 (best 1124531044352.00000), saving model to \"/mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=19-val_mse=1124531044352.00.ckpt\" as top 1\n",
      "2022-03-15 01:37:38 - INFO - __main__ - Copy best model from /mnt/c/Users/natra/Documents/Education/UChicago/NLP/n2c2-track2-nlp-uchicago/output/sl_linear/version_20220315-013303/checkpoints/epoch=19-val_mse=1124531044352.00.ckpt to output/sl_linear/version_20220315-013303/checkpoints/best_model.ckpt.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2022-03-15 01:37:39 - INFO - __main__ - Loading dev features and labels from data/dev_roberta_features.npz and data/dev_stay_len_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97ec86afcf848fbb9d38a0fb9e5c357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_loss': 1124531044352.0,\n",
      " 'val_mae': 602737.5625,\n",
      " 'val_mape': 2.5077879428863525,\n",
      " 'val_mse': 1124531044352.0,\n",
      " 'val_r2': 2.226785898208618}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 01:37:41 - INFO - __main__ - Loading test features and labels from data/test_roberta_features.npz and data/test_stay_len_labels.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (11473, 764)\n",
      "labels shape: (11473,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f223e55b52744c694936d6cfe99ff27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 1064722104320.0,\n",
      " 'test_mae': 607866.5625,\n",
      " 'test_mape': 2.314664125442505,\n",
      " 'test_mse': 1064722104320.0,\n",
      " 'test_r2': 2.167832612991333}\n",
      "--------------------------------------------------------------------------------\n",
      "Validation MSE on the best model:  1124531044352.0000\n",
      "Validation R2 on the best model:  2.2268\n",
      "Test MSE on the best model:  1064722104320.0000\n",
      "Test R2 on the best model:  2.1678\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load hyperparameters\n",
    "parser = ArgumentParser()\n",
    "parser = FeatureBasedRegressionModule.add_model_specific_args(parser)\n",
    "\n",
    "# NOTE: You should replace `unigram_binary` in the assignment statement of `args_str =...`\n",
    "# with whatever feature that you are experimented with.\n",
    "# You can also configure other options listed in the method of add_model_specific_args of\n",
    "# the pytorch-lightning model `FeatureBasedBinaryClassificationLModule`.\n",
    "args_str = (\"--feature_name roberta --max_epochs 20 --label_name stay_len \"\n",
    "            \"--output_dir output/sl_linear --optimizer adam --do_train --do_predict \"\n",
    "            )\n",
    "\n",
    "args = parser.parse_args(args_str.split())\n",
    "\n",
    "# If output_dir not provided, a folder is generated\n",
    "if args.output_dir is None:\n",
    "    args.output_dir = str(\n",
    "        Path('output').joinpath(\n",
    "            f\"{args.task}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"))\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Parsed arguments: {args}\")\n",
    "\n",
    "training_outout = generic_train(args=args,\n",
    "                                model_class=FeatureBasedRegressionModule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[Linear Regression with PyTorch](https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40f6b8985ae3d3af9736205d555f7ff87522357a9f5bdb6e88eda9160976b228"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
